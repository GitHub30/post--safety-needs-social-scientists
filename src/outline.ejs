<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>
<body>

  <h2><todo>Outline</todo></h2>
  <p><d-code block language="markdown">
    - Questions that social science can help us answer
      * Higher quality answers = more aligned AI
      * If we want to target reasoning, the questions we need to answer get more complicated
      * Questions for debate in particular
        * How skilled are people as judges on average?
        * Can we distinguish good judges from bad judges?
        * Does judge ability generalize across domains?
        * Can we train people to be better judges?
        * What questions are people better at answering?
        * Are there ways to restrict or structure debates to make them easier to judge?
        * How can people work together to improve quality?
      - Social scientists required to answer these
      - Our expertise is just ML, so we can't fully articulate what experiment we need
      - Need to bring in people with different backgrounds!

    - Reasons for optimism
      - This is about reasons for optimism about the social science research program
        - See the debate paper for more details about debate specifically
      - Engineering vs. science
        - We have a lot more control over the setup than traditional social science experiments
      - Don't need to answer all questions
        - If we can recognize areas of uncertainty, can push ML models to treat those areas with care
      - Relative accuracy may be enough
        - If debate structure A performs reliably better than debate structure B, that's good evidence
          that we should use A even if we don't know how performance will change over time / with
          more advanced agents
        - Though it'd be great to have absolute accuracy too. :)
      - Don't need to pin down the best debate scheme exactly
        - If we narrow down to a smaller design space, we can test that space once that machines are ready
      - A negative result would be important!
        - If people aren't good enough as judges as soon as possible, we want to know this!
        - Debate isn't the only approach to alignment

    - Reasons to worry
      - Desiderata are conflicting
        - Hard to pick a task that is sufficiently interesting, verifiable, not too easy, etc., etc.
      - Want to measure judge quality w.r.t. optimal debaters
        - But all we have is humans!
        - Inner / outer optimization structure: train debaters then measure judges
        - Makes experiments harder to run, requires more generalization to work
          - I.e., may need to assume that good debates on question X are also good at question Y.
      - Lack of philosophical clarity
        - Debate is both a proposed definition of alignment and an algorithm
        - We don't expect humans to conform to any philosophically consistent scheme
          - We are not cores of utility functions wrapped in shells of irrationality
            - From http://rationallyspeakingpodcast.org/show/rs-219-jason-collins-on-a-skeptical-take-on-behavioral-econo.html
            - Sugden, Looking for a psychology for the inner rational agent
              - https://ueaeprints.uea.ac.uk/54622/1/psychology_of_inner_agent_1506_01.pdf
      - ML algorithm side could change a lot
        - Human experiments would need to adapt to match
        - This is another kind of generalization: hopefully results mirroring one algorithm generalize to others,
          but it isn't guaranteed
      - Need strong out-of-domain generalization
        - Pure human experiments not a perfect match to ML+human situations
        - Don't have any advanced AI systems to play with, and we want to learn things that work for AGI

    - Scale: 1,000s to 10,000s of people
        - Might need a lot of samples to train AGI reliably
        - Lots of samples means lots of judges
        - Lots of judges means lots of researchers to maximize quality
        - Sample complexity concerns: maximizing information per sample
        - Need close collaborations across a variety of disciplines

    - How you can help
      - Please contact us!  Interesting in conversation and close collaboration
        - Probably talk about authors rather than OpenAI as an institution
      - Encourage other safety researchers to think about the human side
  </d-code></p>
</d-article>
</body>
