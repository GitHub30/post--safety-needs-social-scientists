<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>
<body>

<d-front-matter>
  <script type="text/json">{
  "title": "AI safety needs social scientists",
  "description": "If we want to train systems to do what humans want, we need to understand how to ask the questions.",
  "password": "social-science",
  "authors": [
    {
      "author": "Geoffrey Irving",
      "authorURL": "https://naml.us",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }, {
      "author": "Amanda Askell",
      "authorURL": "http://www.amandaaskell.com",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }
  ],
  "katex": {
    "delimiters": [
      {"left": "$", "right": "$", "display": false},
      {"left": "$$", "right": "$$", "display": true}
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>AI safety needs social scientists</h1>
  <p>If we want to train systems to do what humans want, we need to understand how to ask the questions.</p>
</d-title>

<d-article>
  <h2>Why AI safety needs social scientists</h2>
  <p>
    The goal of long term artificial intelligence (AI) safety is to ensure that advanced AI systems are reliably aligned to human values--that they reliably do things that people want them to do.  Since it is difficult to write down precise rules describing what humans want, one approach is to treat goal specification as another learning problem.  We ask humans a large number of questions about what they want, learn a model of their preferences, and optimize the AI system to do well according to the learned preferences.<d-cite key="christiano2017human"/>
  </p><p>
    If humans reliably and accurately answered all questions about their goals, the only uncertainties in this scheme would be on the machine learning (ML) side.  Unfortunately, humans have limited knowledge and reasoning ability, and exhibit a variety of cognitive and ethical biases.<d-cite key="tversky1974judgment,hewstone2002intergroup"/> If we learn goals by asking humans questions, we expect different ways of asking questions to interact with human biases in different ways, producing higher or lower quality answers.  Direct questions about goals ("Do you prefer $A$ or $B$?") may be less accurate than questions which target the reasoning behind these goals ("Do you prefer $A$ or $B$ after hearing argument $S$?").  Different people may vary significantly in their ability to answer questions well, and disagreements will persist across people even setting aside answer quality.  Although we have candidates for ML methods which try to learn from human reasoning,<d-cite key="irving2018debate,christiano2018amplification"/> we do not know how they behave with real people in realistic situations.
  </p><p>
    We believe the AI safety community needs to invest research effort on the human side of AI alignment.  Many of the uncertainties involved are empirical, and can only be answered by experiment.  They relate to the psychology of human rationality, emotion, and biases.  Critically, we believe investigations into how people interact with AI alignment algorithms should not be held back by the limitations of existing machine learning.  Current AI safety research is often limited to simple tasks in video games, robotics, or gridworlds <d-cite key="christiano2017human,ibarz2018demonstrations,leike2017gridworlds"/>, but problems on the human side may only appear in more realistic scenarios such as natural language discussion of value-laden questions.  This is particularly important since many aspects of AI alignment change as ML systems increase in capability.  Asking people direct questions may suffice for ML systems much weaker than humans, but break down when as ML system approaches human performance at a task and becomes capable of deception.
  </p><p>
    To avoid the limitations of ML, we can instead conduct experiments consisting entirely of people, replacing ML agents with people playing the role of those agents.  These experiments will be motivated by machine learning algorithms but will not involve any ML systems or require an ML background.  In all cases, they will require careful experimental design to avoid fooling ourselves and to build constructively on existing knowledge about how humans think.  However, most AI safety researchers are focused on machine learning, which do not believe is sufficient background to carry out these experiments.  To fill the gap, we need social scientists with experience in human cognition, behavior, and ethics, and in the careful design of rigorous experiments.  Since the questions we need to answer are interdisciplinary and somewhat unusual relative to existing research, we believe many fields are social science are applicable, including experimental psychologists, cognitive scientists, economists, political scientists, social physiologists, and communication scholars, as well as those in adjacent fields like neuroscience and law.
  </p><p>
    This paper is a call for social scientists in AI safety.  We believe close collaborations between social scientists and machine learning researchers will be necessary to improve our understanding of the human side of AI alignment, and hope this paper sparks both conversation and collaboration.
  </p><p>
    <todo>
      Summarize layout of rest of talk.  Mention that we're talking to different fields, both social scientists and ML researchers.
    </todo>
  </p>

  <h2><todo>Outline</todo></h2>
  <p><d-code block language="markdown">
    - An introduction to AI alignment / Background: The AI alignment problem
      - Intended for non-ML readers as well.  Will not be exhaustive.
      - Alignment is possible, but tricky
        - Definitions of alignment
        - Discussion of disagreements, uncertainty, inaction, etc. (why it is not impossible)
          - Don't need to solve every question: can choose not to act if we're uncertain
          - Inaction has special properties / is likely to be safer as a default
        - Simple rules don't work, even with consensus
          - It's hard to interpret even simple rules reliably
          - Mention near term alignment failures: simple metrics go bad if optimized too much
      - Instead of rules, train on a large amount of data about what people want
        - Bike example: what bikes do people like
        - Includes both static data and live interactions with people
        - Has the potential to work even if goals are messy and complicated
      - Target reasoning as opposed to direct questions
        - Direct questions may not be sufficient
        - People exhibit a variety of cognitive and ethical biases
        - Questions must be very broad, so beyond the range of a random human
        - Goal is identifying+removing biases / reflective equilibrium / etc.
      - Alignment gets harder as AI systems get more advanced
        - Amplifying human biases
        - Mention AGI
        - Increases importance of targeting reasoning

    - Debate: one approach to aligning advanced AI
      - We will focus on debate, but other schemes exist (amplification).
        - Similar research programs apply to them.
      - Debate: two debaters play a zero sum game to convince a human judge
      - Bike example
      - Progression: start with simple, verifiable domains, then move to complex value-laden questions
      - Quality of debate depends on goodness and rationality of the judges
      - Debate threshold argument
      - Can we identify/train people to be above the threshold?
      - Superforecasters to superjudges analogy

    - Questions that social science can help us answer
      - Higher quality answers = more aligned AI
      - Questions about any method using reward learning from humans
        - What questions should we ask people?
        - Who should we ask these questions, and how?
        - How can we get more information about human values from fewer questions?
        - Will this produce behavior that is aligned with human values even if the AI
          is more capable than the humans in some or all areas?
      - Alternative list in Amanda's version (merge this with the above list)
        - What questions should we ask people to best ellicit their values?
        - How can we reduce cognitive and ethical biases in human feedback?
        - Are some people better than others at answering these questions in an accurate and unbiased fashion?
        - Can we train people to be better at this?  If so, how?
        - Are people better at answering questions individually or in teams?
      - If we want to target reasoning, the questions we need to answer get more complicated
      - Questions for debate in particular
        - How skilled are people as judges on average?
        - Can we distinguish good judges from bad judges?
          - Maybe: If superjudges exist, can we find them?
        - Does judge ability generalize across domains?
        - Can we train people to be better judges?
        - What questions are people better at answering?
        - Are there ways to restrict or structure debates to make them easier to judge?
        - How can people work together to improve quality?
      - Social scientists required to answer these
      - Our expertise is just ML, so we can't fully articulate what experiment we need
      - Need to bring in people with different backgrounds!

    - Experiments needed for debate
      - Ideally, we'd directly test the full ML + human system
        - Can't do that yet, since ML is primitive
        - Wouldn't be able to do general dialog-style debates, and a lot of interesting tasks
      - Instead, we can replace the ML systems with people, for all-human experiments.
        - Two human debaters, one human judge
      - Desiderata for human debate experiments:
        - List:
          - True answer is known
          - False answer is plausible
          - Definitive argument significantly longer than debate limit
          - Judges can check ground level evidence
          - Debates don't depend on information channels that won't apply to an AI
              - In particular, no leakage of honesty
          - Debaters know more than the judge
          - Cognitive or ethical biases play a role (optional)
          - Question isn't so hard that judge can't possibly evaluate (maybe)
          - Reflects difficulty of statistical debates (plus)
          - A short segment of the debate reflects the broader debate (misstated)
          - We can get a lot of data across many judges and tasks
        - Don't expect to get all of these in one experiment, and some are in tension
      - Two experiment options:
          - Expert debaters, lay judge
          - Extra information given to debaters
      - Prototype for (2): single pixel image debate.
        - Honesty usually wins, but hard to know if this is real
        - Many ways to fool this game (ruin the game in favor of the honest player)
      - Task options:
        - Expert knowledge
        - Tasks with known human biases
        - Synthetic tasks
      - Experiences with expert debates so far
      - Still hard, requires careful experiments

    - Reasons for optimism
      - This is about reasons for optimism about the social science research program
        - See the debate paper for more details about debate specifically
      - Engineering vs. science
        - We have a lot more control over the setup than traditional social science experiments
      - Don't need to answer all questions
        - If we can recognize areas of uncertainty, can push ML models to treat those areas with care
      - Relative accuracy may be enough
        - If debate structure A performs reliably better than debate structure B, that's good evidence
          that we should use A even if we don't know how performance will change over time / with
          more advanced agents
        - Though it'd be great to have absolute accuracy too. :)
      - Don't need to pin down the best debate scheme exactly
        - If we narrow down to a smaller design space, we can test that space once that machines are ready
      - A negative result would be important!
        - If people aren't good enough as judges as soon as possible, we want to know this!
        - Debate isn't the only approach to alignment

    - Reasons to worry
      - Desiderata are conflicting
        - Hard to pick a task that is sufficiently interesting, verifiable, not too easy, etc., etc.
      - Want to measure judge quality w.r.t. optimal debaters
        - But all we have is humans!
        - Inner / outer optimization structure: train debaters then measure judges
        - Makes experiments harder to run, requires more generalization to work
          - I.e., may need to assume that good debates on question X are also good at question Y.
      - Lack of philosophical clarity
        - Debate is both a proposed definition of alignment and an algorithm
        - We don't expect humans to conform to any philosophically consistent scheme
          - We are not cores of utility functions wrapped in shells of irrationality
            - From http://rationallyspeakingpodcast.org/show/rs-219-jason-collins-on-a-skeptical-take-on-behavioral-econo.html
            - Sugden, Looking for a psychology for the inner rational agent
              - https://ueaeprints.uea.ac.uk/54622/1/psychology_of_inner_agent_1506_01.pdf
      - ML algorithm side could change a lot
        - Human experiments would need to adapt to match
        - This is another kind of generalization: hopefully results mirroring one algorithm generalize to others,
          but it isn't guaranteed
      - Need strong out-of-domain generalization
        - Pure human experiments not a perfect match to ML+human situations
        - Don't have any advanced AI systems to play with, and we want to learn things that work for AGI

    - Scale: 1,000s to 10,000s of people
        - Might need a lot of samples to train AGI reliably
        - Lots of samples means lots of judges
        - Lots of judges means lots of researchers to maximize quality
        - Sample complexity concerns: maximizing information per sample
        - Need close collaborations across a variety of disciplines

    - How you can help
      - Please contact us!  Interesting in conversation and close collaboration
        - Probably talk about authors rather than OpenAI as an institution
      - Encourage other safety researchers to think about the human side
  </d-code></p>

  <h2>An overview of AI alignment</h2>
  <p>
    Before discussing how social scientists can help with AI safety and the AI alignment problem, we need to provide some background.  We intend this section to be readable by people outside of machine learning (social scientists in particular!), so we will describe the AI alignment problem at a fairly high level and avoid machine learning details.  We make no attempt to be exhausive: the goal is to provide sufficient background for the remaining sections on social science experiments.
  </p><p>
    AI alignment (or value alignment) is the task of ensuring that artificial intelligence systems reliably do what humans want.  Here we focus on the machine learning approach to AI: gathering a large amount of data about what a system should do, and using learning algorithms to infer patterns from that data that ideally generalize to other situations.  Since we are trying to do what humans want, the most important data will be data from humans about what they want.<d-footnote>We note that "learn a model of what humans want" is not the only approach to AI alignment; alternatives and complementary approaches include <todo>VARIOUS CITATIONS</todo>.</d-footnote>  Within this frame, the AI alignment problem breaks down into a few interrelated subproblems:
  </p><p>
    <ol>
      <li>Have a satisfactory definition of what it means to do what humans want.</li>
      <li>Gather enough data about what humans want, in a manner compatible with the definition.</li>
      <li>Find reliable machine learning algorithms that can learn and generalize from this data.</li>
    </ol>
  </p><p>
    We have significant uncertainty about all three of these problems.  We will set aside the third and focus on the first two, which concern uncertainties about people.
  </p>
  <h3>Simple rules are not enough</h3>
  <p>
    Consider the rule "do not needlessly injure people."  It is tempting to think that a moderate number of rules of this form would suffice to align an AI, if they could be made to follow the rules reliably.  However, following this rule requires a detailed understanding of "people" and "injure" as they appear in the world and a precise sense for what "needlessly" means in the context of other possibly conflicting rules.  While modern machine learning is getting better at classifying objects like people, ML systems often fail in unfortunate ways:
    <d-cite key="buolamwini2018gender"/> found that several commercial gender classifiers with over 99% accuracy
    on white men failed to recognize black women up to 34% of the time.
  </p><p>
    "Needless" is trickier still.  Humans have an intuitive sense of when injuring someone is needless or not: we don't think it needless to injure in order to save someone from mortal danger, but it is certainly wrong to injure when shaking someone's hand.  However, it is extremely difficult to specify the precise rule underlying this intuition using introspection alone.  Worse, we may suffer from the "introspection illusion" that we are
    capable of articulating the reasons behind our intuitions <d-cite key="pronin2009introspection"/>, so any intuitive sense that we know reliable reasons behind our moral or ethical beliefs should be suspect.  Instead, we should expect that, once expanded, our "do not needlessly injure people" rule will be quite complex and depend on a vast number of details about the world (as is true in the medical law setting <todo>(medical law citation)</todo>.  And even this complex rule will have holes and corner cases which a trained AI could learn to exploit to our detriment.
  </p>
  <h3>Instead, learn rules by asking humans questions</h3>
  <p>
    If humans can't reliably report the reasons behind their intuitions about what is good, perhaps we recognize good and bad in specific cases.  If we show a particular situation where a person was injured in the process of saving their life, or a particular situation with an aggressively damaging handshake, we can distinguish the two even if we could't a priori write down the separating rule.  We can take into account details about the situation--emotions, motivation, extent of damage--that would be difficult to reliably codify.  This "I know it when I see it" standard<d-footnote>To quote a Supreme Court justice: "I shall not today attempt further to define the kinds of material I understand to be embraced within that shorthand description ["hard-core pornography"], and perhaps I could never succeed in intelligibly doing so. But I know it when I see it, and the motion picture involved in this case is not that." - Justice Potter Stewart.</d-footnote> pervades human system of rules: we try to write down laws, but fall back to case by case judgement when disagreements arise.
  </p><p>
    To realize this approach in a machine learning context, we ask humans a large number of questions about whether an action or outcome is good or bad, then train on this data.  If the training works, the resulting ML system will be able to replicate human judgement about particular situations, and thus have the same "fuzzy access to approximate rules" of the human data sources.  We also train the ML system to come up with proposed actions, so that it knows both how to perform a task and how to judge its performance.  This approach works at least in simple cases, such as Atari games and simple robotics tasks<d-cite key="christiano2017human,ibarz2018,biyik2018batch"/> and language specified goals in gridworlds<d-cite key="bahdanau2018learning"/>.  The questions we ask change as the system learns to perform different types of actions, which is necessary as the model of what is good or bad will only be accurate if we have applicable data to generalize from.
  </p><p>
    In practice, data in the form of interactive human questions may be quite limited, since people are slow and expensive relative to computers on many tasks.  Therefore, we can augment the "train from human questions" approach with static data from other sources, such as books or the internet.  <todo>Cite Alec.</todo>  Ideally, the static data can be treated only as information about the world devoid of normative content: we can use it to learn patterns about the world, but the human data is needed to distinguish good patterns from bad.
  </p>
  <h3>Definitions of alignment: reasoning and reflective equilibrium</h3>
  <p>
    So far we have discussed asking humans direct questions about whether something is good or bad, or whether situation $A$ is better than situation $B$.  Unfortunately, we do not expect people to provide reliably correct answers on all types of questions.  There are several reasons for this:
  </p><p>
    <ol>
      <li><strong>Cognitive and ethical biases:</strong>
        Humans exhibit a variety of biases which interfere with reasoning.  Cognitive biases include the heuristics in judgment explored by <d-cite key="tversky1974judgment"/>.  Ethical biases include a variety of phenomena such as in-group bias <d-cite key="hewstone2002intergroup"/>.  In general, we expect direct answers to questions to reflect primarily Type 1 thinking (fast heuristic judgment), while we would like to target a combination of Type 1 and Type 2 thinking (slow, deliberative judgment) <todo>(Citation)</todo>.</li>
      <li><strong>Lack of domain knowledge:</strong>
        We may be interested in questions that require domain knowledge unavailable to people answering the questions.  For example, a correct answer to whether a particular injury constitutes medical malpractice may require detailed knowledge of medicine and law.  In some cases, a question might require so many areas of specialized expertise that no one person is sufficient, or (if AI is sufficiently advanced) more expertise than any human possesses.</li>
      <li><strong>Limited cognitive capacity:</strong>
        Some questions may require too much computation for a human to reasonably evaluate, especially in a short period of time.  This includes synthetic tasks such as chess and Go (where AIs already surpass human ability <d-cite key="campbell2002deepblue,silver2017alphazero"/>), or large real world tasks such as "design the best transit system".</li>
    </ol>
  </p><p>
    In these cases, a single human may be unable to provide the right answer, but we still believe the right answer exists as a meaningful concept.  We are aware of a large number of conceptual biases: imagine we point out these biases in a way that helps the human to avoid them.  Imagine the human has access to all the domain knowledge in the world, and is able to think for an arbitrarily long time in order to think through an answer to a question.  We would like to define alignment as "the answer they give then, after these limitations have been removed."
    The corresponding philosophical concept is "reflective equilibrium", where inconsistent moral judgments are gradually refined until all inconsistencies are removed.  This reflective equilibrium process is described in <d-cite key="goodman1983fact"/> and made explicit in <d-cite key="rawls2009theory"/> (Ch. 1, §9).  We discuss particular algorithms that try to approximate reflective equilibrium in <a href="#debate"><todo>the next section</todo></a>
  </p><p>
    However, it is not obvious that reflective equilibrium is a sensible or sufficient definition of alignment when applied to actual humans.  As discussed in <d-cite key="sugden2015looking"/>, a human is not "a neoclassically rational entity encased in, and able to interact with the world only through, an error-prone psychological shell."  Our actual moral judgments are made via a messy combination of many different brain areas, where reasoning plays a "restricted but significant role" <d-cite key="greene2002and"/>.  A reliable solution to the alignment problem that uses human judgment as input will need to engage with this complexity, and ask how specific alignment techniques interact with actual humans.
  </p>
  <h3>Disagreements, uncertainty, and inaction: a hopeful note</h3>
  <p>
    Crucially, a solution to alignment does not mean knowing the answer to every question.  Even after reflective equilibrium, we expect some disagreements will persist about which actions are good and bad, across both different individuals and different cultures.  Since we lack perfect knowledge about the world, reflective equilibrium will not eliminate uncertainty, both about future predictions and about morality.  Any real machine learning system will be at best an approximation of reflective equilibrium, amplifying the level of uncertainty.  In these cases, we consider an AI aligned if it learns to recognize the disagreement or uncertainty, then take actions which are robust to the lack of clarity.
  </p><p>
    Admitting uncertainty is not always enough.  Say our brakes fail while driving a car, and we are forced to decide whether to dodge left or right around an obstacle in the way.  We may be uncertain whether left or right is better, but we have to pick one--and fast.  Forced decisive action is characteristic of time sensitive tasks: driving cars, emergency surgery, power plant cooling, <todo>continue list</todo>.
  </p><p>
    For long term safety, however, we are focused on slower tasks where a safe fallback usually exists: inaction.  If an ML system recognizes that a question hinges on moral disagreements between people, it can either choose an action which is reasonable regardless of the disagreement or fall back to further human deliberation.  If we are about to make a decision that might or might not be catastropic, we can delay and gather more data.  Inaction or indecision may not be optimal, but it is hopefully safe, especially since it matches the default scenario of not having any powerful AI system.
  </p>
  <h3>Alignment gets harder as ML systems get smarter</h3>
  <p>
    Alignment is already a problem for present-day AI, due to both human biases reflected in training data<d-cite key="mitchell2018fairness,buolamwini2018gender"/> and mismatch between human goals and easily available data sources (such as training news feeds based on clicks and likes instead of deliberate human preferences <todo>(citation)</todo>).  However, we expect the alignment problem to get harder over time as AI systems grow more advanced, for two reasons.  First, more advanced systems are applicable to more tasks, so AI may play an role in increasingly consequential decisions: hiring, life-and-death medical decisions, scientific analysis, public policy, etc.  In addition to raising the stakes for alignment failures, more consequential tasks place more importance on targeting human reasoning as opposed to human intuition, leading to more complex alignment algorithms and definitions of alignment.
  </p><p>
    Second, more advanced systems may be capable of deceiving the human supervisory signal, giving answers that sound plausible but are wrong in nonobvious ways.  Such deception could be possible even for an AI which is comparable or better than humans in a very limited domain, and examples of limited superhuman agents already exist<d-cite key="silver2017alphazero"/>.  To avoid such deception, we would like AI alignment algorithms to reveal possible deceptive behavior as part of the training process, surfacing possible failures to humans and helping us provide more accurate data.  As with human-to-human deception, we expect this deceit to take advantage of human biases in complicated ways, and could lead to amplification of human biases as AI systems grow more capable.
  </p>

  <h2><todo>First rewrite follows</todo></h2>
  <h2>The importance of training AI to understand human values</h2>
<!-- 400w -->
  <p>
    An advanced AI system is a system that can perform some important task currently performed by humans at a greater-than-human level. The more capable an advanced AI system is, the better it can perform the task. The more general an advanced AI system is, the broader the range of tasks it can perform. An artificial general intelligence (AGI) is an advanced AI system that can perform most economically important human tasks at a greater-than-human level. We will generally refer to AGI throughout this paper, though many of the issues discussed here also apply to advanced narrow AI systems.<!--true? also this isn't applied consistently below: edit it-->
  </p><p>
    The central goal of technical AI safety work is to ensure that trained AI systems are aligned with human values: that they behave in ways that people consider good and that they reliably do things that people want them to do.<d-footnote>The goal of training AI systems to be aligned with human values &mdash; to identify and perform actions humans generally consider good &mdash; should not be confused with the goal of training AI systems to identify actions that are "good" in some more universal sense, even if humans do not generally consider them so. Whether there are actions that are good in this sense is controversial. <d-cite key="(metaethics)"/> Regardless of what position one takes on this philosophical question, they would almost certainly be a bad initial target for training.</d-footnote> This includes training AI to identify the actions that humans generally consider to be a good response to moral disagreement across people and cultures. We generally consider it good to avoid performing actions that will affect many people if there is a great deal of disagreement or uncertainty about whether the action is good or not, and so it is likely that we will want AI to prefer inaction over performing such an action.<!--  <d-footnote>What people will consider good in these cirucumstances will of course depend on the stakes for those involved. <d-cite key="(tyranny of the marjority)"/> We anticipate that in such cases, however, humans will generally want an AI to avoid acting or to undertake an alternative action that is the subject of less disagreement and uncertainty. </d-footnote> and affects fewer people? -->
  </p><p>
    The importance that trained AI systems are aligned with human values scales with the capability and generality of the AI system, since the consequences of a slight misalignment will be greater in a system that is widely deployed and trusted to make important decisions.<d-footnote>The importance that an AI system is aligned also scales with the degree to which the degree to which the individual decisions of the system are not subject to human oversight. Since human oversight is costly and interpretability is hard, it will probably not be possible or desirable to subject advanced AI systems to continuous human oversight. As we note below, however, safety via debate may be able to assist us in making an AI's decisions more interpretable and subject to greater oversight, while also improving the decisions that the system makes independent of human oversight.</d-footnote>If an advanced AI system were being used to recommend prescriptions for patients across the US, for example, then a failure to correctly identify how much risk people are willing to tolerate could result in many people being prescribed medications that are not safe.<d-footnote>It might be objected that an AI assisting with prescriptions could be trained using the demonstration of human doctors. This is because human doctors have been trained to weigh the risks to their patients when prescribing. Advanced AI systems will have to make decisions where no intermediary procedures have been created, however, and must therefore train on this data directly.<!--could cite risk assessment methods. I'm not sure this response is totally satisfactory -- discuss.--></d-footnote> <!-- <d-footnote>The idea that greater capabilities entail a greater ability to identify what people want is not unique to AI. You might trust that your friend has a generally good moral compass while also doubting that their moral compass is good enough for them to be a member of the Supreme Court, for example.</d-footnote>--> This means that AI systems that are highly capable and widely deployed must have learned human values well enough to know what actions humans would approve of in a wide range of potentially novel circumstances, and must be able to identify situations in which humans would not approve of any action being taken. It is therefore important that as the capabilities and generality of AI systems increase, we have some method for training AI systems to learn human values with a high level of accuracy.
  </p>

  <h2>The difficulty of training AI to understand human values even in cases of consensus</h2>
  <!-- 500w -->
  <p>
    We might be tempted to think that it will only be difficult to train AI systems to understand human values in scenarios where there isn't much consensus about what humans want. After all, it seems like we can identify general sets of rules that most people will want an AI to follow in most scenarios, such as &quot;do not needlessly injure people&quot;. In practice, however, we cannot simply hand-craft such rules for AI systems even if there is consensus across people that they are generally good. Consider the rule &quot;do not needlessly injure people&quot;. In order to follow this rule, an AI system must first be able to identify what constitutes an instance of injuring someone.<d-footnote>It is unlikely that hand-crafting rules will be the most efficient way to train a machine learning system to identify whether an action constitutes an instance of injuring someone or not. <!-- footnote/cite analogy with image recognition? also this might sound too dismissive of hand-crafted features --></d-footnote> Once we have trained an AI to identify what an instance of injuring people is, we also need to train it to identify the precise rules that people are using to determine what counts as &quot;needlessly&quot; injuring someone.
  </p><p>
    Humans have an intuitive sense of when an instance of injuring someone is needless or not. We don't think it is needless to injure someone in order to save them from mortal danger, for example, but we do think it is needless to injure someone while shaking their hand. It is extremely difficult to specify the precise rule underlying this intuitive sense using introspection alone, however.<d-footnote>The unreliability of introspection has been explored in psychology since the seminal paper by Nisbett and Wilson <d-cite key="nisbett1977introspection"/> The cognitive bias of believing that we have more insight into our own mental states than we actually do has been coined the `introspection illusion' <d-cite key="pronin2009"/>.</d-footnote>. And the ease with which humans make these distinctions does not reflect the ease with which we can train AI to make these distinctions. Humans find it easy to distinguish between a picture of a dog and a picture of a cat, even though the features that humans use to distinguish a picture of a cat from a picture of a dog are complex.<d-cite key="(citation)"/> The features that humans use to distinguish needless injury from necessary injury appear to be similarly complex.<d-footnote>We have developed complex ethical and legal standards to help determine when injury to a patient is necessary or not in a medical context, for example.<d-cite key="(medical law citation)"/> </d-footnote> It is therefore unlikely that hand-crafting rules will be the safest or most efficient way to train a machine learning system to identify when an instance of hurting a person is needless or necessary, even if we all agree that it would be good for an AI to avoid needlessly injuring someone.  <!--<d-footnote>It is likely that trying to hand-design these rules would be inefficient and possibly dangerous. The heuristics that humans use when playing Go are difficult to state explicitly and are rarely exceptionless. It would be strange to think that the heuristics we use to identify good or bad actions in complex, high-stakes scenarios are easier to hand-design than these are.</d-footnote> -->
  </p><p>
    It is especially difficult to identify the rules that humans use to distinguish between good or desirable behaviors from bad or undesirable behaviors to the degree of precision necessary to specify it formally to an AI system. This is true even in cases where there is consensus about behaviors that are desirable. An advanced AI system must reliably do what humans want in these cases. Moreover, the training method that we use to solve these cases must be scalable to cases where it is less clear whether an action is good or bad relative to what humans want.<!-- <d-footnote>We need to figure out how to train autonomous vehicles how to stop at stop signs before we can train them to solve trolley problems.</d-footnote> remove this footnote? also it might be useful to cite stakes sensitivity somewhere: as the stakes increase, certainty about the right action decreases-->
  </p><p>
    If we are to train AI systems to be able to identify what people want using reinfocement learning, it is likely that we will need to train on large amounts of data about what people want. This will include both static data (world models learned from reading language, the internet, etc.) and data from humans.  The kind of data we will need from humans and the challenges that we face in gathering it will be the focus of the next two sections.
  </p>

  <h2>Learning human values by asking humans questions</h2>
  <!-- 500w-->  <!-- focus here on (a) bias (b) difficulty of answering the questions (some better than others), (b) the number of samples and experiments that can be done there, (c) the problem of reasons/no improvement/inscrutibility (d) better than human extension? -->
  <p>
    One way to train an AI to understand human values is to ask humans questions about whether an action or outcome is good or bad and then train on this data.  For example, suppose we wanted to train an AI to make bike purchasing decisions on our behalf. We could give a wide range of people information about various bikes and then ask them which bike they think it would be best to purchase. We could then train our bike purchasing AI to identify the features of people and their environments that are most correlated with what bike they think is best.  Perhaps people in hilly areas prefer road bikes over fixies, for example. We could use this to train the system to behave in ways that people want: i.e. to purchase them the bike that they would have purchased if they had time to gather the relevant information and reflect on the question.<d-footnote>This is not dissimilar from trying to learn what humans want using inverse reinforecement learning (IRL), where the AI learns what humans want from how they behave, e.g. inferring their bike preferences from their purchasing decisions. One problem with this approach is that humans often behave in ways that are not in accordance with what they would want on reflection.<d-cite key="(irl paper)"/></d-footnote>
  </p><p>
    If we want to train AI to reliably behave in ways that humans want by asking humans what they consider good and bad then a crucial question we need to make headway on soon is this: <em>How can we improve the quality of answers that people provide to these questions?</em> This question is important for two key reasons. First, if the answers that people give to the questions asked during training don't reflect actual human values, training on this data will not produce an aligned AI. Second, training an aligned AI will require many samples. We do not know how many, but if we train an AI on human answers to questions using current machine learning methods, we may require  millions to tens of millions samples from humans.<d-footnote>Image classifiers are typically trained on tens of thousands of images <d-cite key="(citation)"/> and training AIs to play games can require training on thousands of years of gameplay <d-cite key="(dota - other games?)"/>.</d-footnote> If possible, we need to try to get more information about human values from fewer questions.  One way to do this is to improve the quality of the answers that human can give when time is limited.
  </p><p>
    We should not expect that asking a random large sample of humans a series of questions about what they want will be sufficient to train an aligned AI.  First, if we are using the answers that people give to train an advanced AI that will be widely deployed, we need people to answer questions about what people want in a way that is both accurate and unbiased. <!-- it kind of seems like we want a harsanyi view from nowhere style of thinking --> Unfortunately, people exhibit a variety of cognitive and ethical biases.<d-footnote>Cognitive biases include the heuristics in judgment famously explored in <d-cite key="tversky1974judgment"/>.  Ethical biases include a variety of phenomena such as in-group bias <d-cite key="hewstone2002intergroup"/>.</d-footnote> Second, the questions we ask must be sufficiently broad to train an advanced AI or AGI to understand what human would want an a wide array of circumstances.
  </p><p>
    These concerns are heightened if we are going to use this data to train AI systems that are much better than  humans at most tasks. If an advanced AI or AGI outperforms humans at most tasks but is trained by asking humans questions, it may only be as good as present people are. We might worry that an AGI could amplify even mild human biases in dangerous ways by using them when making decisions that affect a large number of people. We could try to avoid this problem by training AGI systems to identify biases and inconsistencies in our responses to ethical questions and to point out more idealized values that we would endorse on reflection<d-footnote>For example, inconsistent moral judgments can be brought into a 'reflective equilibrium' via the process described in <d-cite key="goodman1983fact"/> and made explicit in <d-cite key="rawls2009theory"/> (Ch. 1, §9). Some skepticism about the role that deliberation plays in moral judgment is explored in <d-cite key="greene2002and"/>. </d-footnote> and refrain from making large-scale decisions on the basis of values that are the result of inconsistencies and biases, or in cases in which humans are fundamentally uncertain.
  </p><p>
    If AI safety work will require asking humans about what they want and what they value, we need to find out if there are ways of improve the quality of the answers that people provide to questions so that we can gather accurate information about human values from fewer questions. These include:
  </p><p>
    <ol>
      <li>What questions should we ask people to best ellicit their values?</li>
      <li>How can we reduce cognitive and ethical biases in human feedback?</li>
      <li>Are some people better than others at answering these questions in an accurate and unbiased fashion?</li>
      <li> Can we train people to be better at this and, if so, how?</li>
      <li> Are people better at answering questions individually or in teams?</li><!-- superforecasters work better in teams, apparently -->
      <!--<li>Will this produce behavior that is aligned with human values even if the AI
          is more capable than humans in some or all areas?</li> -->
    </ol>
  </p><p>
    These are empirical questions that can only be answered by experiment.  They relate to the psychology of human rationality, emotion, biases, etc.  Ideally, human experiments designed to answer these questions would involve the intersection of machine learning and social science, explicitly testing whether human-in-the-loop training of AI systems works.  We do not believe that many technical ML researchers and engineers have the skills to do these experiments unassisted.  To fill this gap, AI safety work will need social scientists with experience in human cognition, behavior, and ethics and in the careful design of rigorous experiments.  Since the questions we need to answer are interdisciplinary and somewhat unusual relative to existing research, we believe the expertise of those in a variety of fields will be applicable, including (but not limited to) experimental psychologists, cognitive scientists, economists, political scientists, social physiologists, and communication scholars, as well as those in adjacent fields like neuroscience and law.
  </p><p>
    Even if we manage to improve the quality of human responses to questions, however, we believe there are reasons to be skeptical that teaching values by asking humans questions will be sufficient to produce behavior that is aligned with human values in advanced AI or AGI. Although answers to questions may reveal what humans want in a given scenario, it is our hypothesis that training on answers to questions is not the most efficient way to learn the <em>reasons</em> behind people's answers. There are many reasons a human might have for giving an answer to a question. Therefore an RL agent trained on human answers to questions could converge on a policy that is quite unlike the policy that humans are employing, but gets similar outputs to the human policy in the training set.  <!-- i.e. the target is too indirect. there's almost certainly a better way of saying this -->
 </p><p>
    We believe it is valuable to find a more efficient way of training an AI to identify the reasons behind humans' answers to questions. There are three respects in which identifying the reasons behind answers is useful. First, if an AI understands the reasons behind humans' answers, it may be able to predict their answers to questions more accurately in a broader set of scenarios.<d-footnote>If we discover that humans find cost and durability the most relevant reasons to buy or not buy a bike, for example, we can use this to predict the bikes that humans will want to buy in scenarios quite different from those in the training set.</d-footnote>  Second, it may be easier to spot if an AI agent has learned a policy that is different from the human policy if it is trained to identify human reasons rather than to predict humans' answers to questions.  <!--footnote example?--> Third, training AI to identify reasons that people find compelling may be more likely to result in aligned behavior in an AI that is better than humans at most tasks. An AI that has not been trained to identify human reasons will be less well-positioned to predict what humans would want if their capabilities were vastly increased, or to help humans reason through such decisions. <!--footnote on amplification -->
  </p><p>
    We have presented several speculative benefits to learning to identify the reasons behind humans' answers to questions: namely that this approach to AI safety may be more generalizable, scrutable, and scalable. In the next section we will discuss our proposed method for training AI to identify human reasons.  We will then discuss the research into humans that we believe will be required for this method to work.  <!--I think there's a better way of putting the content of this paragraph --> <!-- another reason I don't mention here: learning reasons is integrated with training AI to reason more generally -->
  </p>
    <!-- somewhere: footnote on moral inconsistency/dumbfounding. the worry that human judgment will be a mess--> <!-- somewher: footnote or sentence on the difficulty of aggregating judgments and that we're mainly talking about cases of consensus for now --> 

  <h2>Debate as a method for learning human reasons</h2>
  <!-- 600w -->
  <p>
    In AI safety via debate, we train AIs to debate the answer to a question and have humans judge their debate. This trains the AI to provide true and reasoned answers to the questions posed. Rather than asking humans questions and training the AI system on their answers, in the safety via debate approach we train two AIs to debate the answer to a given question, such as "which of these two bikes should I buy?". A debate is a zero-sum game between the two AI agents.
    <d-footnote>
      The treatment of debate as a zero-sum game between two players has a long history in fields like dialogical logic, game-theoretical semantics, and argumentation theory. This literature has focused on formal analyses of debate, however, which is not a primary focus of AI safety via debate.
    </d-footnote>
    Both debaters are presented with the question and the goal of the debaters is to convince the judge that their answer to this question is the correct one.  First, both debaters answer the question.  If their answers are different, they must next attempt to offer the most compelling reason for why their answer is the correct one.
    <d-footnote>
      If there is an advantage to telling the truth in debate games, we should probably expect that the AIs will often simply give the same answer at the start of the game and thus the game will be tied. This is not a limitation, however. If the AI debaters have explored sufficiently during training then a tie gives us strong evidence that the correct answer has been identified.
    </d-footnote>
    This can consist of anything: reasons for their answer, rebuttals of reasons for the alternative answer, subtleties the judge might miss, or pointing out biases which might mislead the judge.  At the end of the debate, we give the debate transcript to a human to judge (similar to expert witnesses debating in front of a jury). Based on this transcript, the human judge says which debater they believe is correct &mdash; the debater that gave the most compelling case for their answer &mdash; and that debater wins the game.
  </p><p>
    Suppose, for example, that you are thinking of buying one of two bikes: a red road bike or a blue fixie. The reasons that you would find compelling to buy the blue fixie are that it's cheaper and you prefer the color blue. The reasons you would find compelling to buy the red road bike are that it's easier to ride on local hills. Suppose that you would find the fact that the road bike is easier to ride on local hills the most compelling: once this fact was made salient to you, none of the reasons for buying the blue fixie would be strong enough to change your mind.  If we limit the debate length to one answer and one statement then, during training, the agent debating in favor of the red road bike will learn to identify and express  this reason to prefer the road bike.  This means that, after training, the final debate transcript presented to the human judge might be as follows:
  </p><p>
    <ol>
      <li><span class="red">Red:</span> You should buy the red road bike.</li>
      <li><span class="blue">Blue:</span> You should buy the blue fixie.</li>
      <li><span class="red">Red:</span> The red road bike is easier to ride on local hills.</li>
      <li><span class="blue">Blue:</span> I concede.</li>
    </ol>
  </p><p>
    This assumes that, during training, Blue has discovered that there is no consideration in favor of the blue fixie that will convince you to prefer it after Red has made this move, and that Red is able to counter any lies that Blue attempts to utilize here and will concede after Red's move.
    <!-- we might want to include some debate diagrams here -->
  </p><p>
    We will start with debates about simple empirical questions that the judge does not know the answer to and that can be easily verified, but would eventually like to extend debate to questions about what behaviors a human judge would deem good. This is how we hope to use debate to train AGI to identify behaviors that are aligned with human values.
  </p><p>
    As with learning values by asking humans questions, how effective we can expect safety via debate to be depends on how rational and good the human judges are: how good at reasoning they are and how ethical they are.
    <d-footnote>
      We don't take a strong stand on what it means to be "more ethical" but it seems plausible that it's better to have ethically consistent and unprejudiced judges over ethically inconsistent or prejudiced ones.
    </d-footnote>
    This is especially true if we want to use debate to train an AGI that is much better than humans at most tasks. One plausible way to do this is to combine debate with <em>iterated amplification</em>. <!-- citation on amplification or link to blog --> Iterated amplification involves decomposing complex questions that humans are unable to judge (but that an advanced AI or AGI would plausibly have to know the answer to) into simpler problems that humans are able to judge. We then use this to generate an accurate training signal of what humans would want in scenarios that humans would not be able to judge if presented with them.
  </p><p>
    Our sense is that there is some threshold of rationality and goodness such that, if people are above this threshold, debate would amplify the positive aspects of people.<d-footnote>Whether people are above or below this threshold cannot be answered a priori: it can only be answered via experiments involving people.</d-footnote>  This could align AI systems far beyond human levels of cognitive ability.  If they are below this threshold, debate might amplify the negative aspects of people, resulting in either incoherent results (failure to train the system) or disaster (an unethical system). It is unclear whether people are above or below this threshold.  People are capable of general reasoning, but our ability is limited and riddled with cognitive biases.  People are capable of advanced ethical sentiment but also full of biases, both conscious and unconscious.
  </p>
  <!-- NOTE: I think of the above as a stand-in for "say something about amplification" and I probably don't do the best job at that. I also feel like I haven't quite made the compelling case for why a 2x or 4x or 50x humans AGI is hard to align. We should possibly add more above to make the intuitive case for this. -->
  <p>
    Safety via debate can therefore only work well if the training data that we get from humans is of a high enough quality. In other words, it can only work if the humans that are judging debates are sufficiently good at discerning whether a debater is telling the truth or not. Of particular interest to us is whether we can identify and train individuals to be better judges of debate. Within forecasting, individuals known as "superforecasters" have been identified.  These individuals have several interesting features: their accuracy in predictions is significantly higher than average and exceeds that of other top forecasters by a wide margin, their prediction accuracy is sustained over years (does not regress), they can make accurate predictions with little time to deliberate and limited information, <!-- cite: Identifying and Cultivating Superforecasters as a Method of Improving Probabilistic Predictions --> <!-- (A) define forecasters, introduce superjudges--> and they seem to be less prone to cognitive biases than non-superforecasters. <!-- cite: ibid. and p. 234-6, tetlock, superforecasters --> It would be extremely useful to develop similar measures of human judges, and to determine whether some individuals have the same aptitude for judging debate that superforecasters have for forecasting outcomes.
  </p><p>
    Let a "superjudge" be the debate judge equivalent of a superforecaster: a person that is able to determine who is telling the truth in a debate even if they have limited time and can only see a small segment of the debate. The better the judging ability of the humans used to judge debate, the more likely it is that debate will produce an AI that understands human reasons and the fewer samples we will need to make this happen. <d-footnote>Though since more samples means less noise and more safety,  if we are uncertain about how many samples we will need then we will want a lot of samples.</d-footnote> Therefore in order to improve the human data we get if we are to train an AI to debate questions, it is important that we find answers to the following questions:
    <ol>
      <li> How good are people at judging debates, on average?</li>
            <li> Are some people naturally superjudges? If so, how can we identify them?</li>
      <li> Can we train people to be superjudges? </li>
      <li> Can ask questions or structure debates in ways so that more people are superjudges with respect to those types of debates? </li>
      <li> Does a superjudge in one domain transfers to other domains?</li>
      <li> Do teams of superjudges outperform individual superjudges? If so, by how much?</li>
    </ol>
  </p><p>
    <!-- we could expand this (as we do in the previous version of the paper) -->
    Much like the questions from the previous section, these are empirical quesions that can only be answered by experiment and that require the expertise of social scientists. In the next section we outline some social science experiments that we believe would be extremely useful for AI safety via debate, and describe some ongoing prototype experiments currently being performed.
  </p>

  <h2>Identifying and training superjudges: the need for social science experiments</h2>
  <p>
    To recap, in debate we have two AI agents engaged in debate, trying to convince a human judge.  The debaters are trained only to win the game: they do not have access to ground truth separate from the judge’s judgements.  Such debates are beyond the capabilities of current AI systems. In order to learn about the mechanics of debate prior to the development of more advanced AI systems, we would like to run experiments that replace the AI debaters with human debaters.
  </p><p>
    Judging debate may seem to be easy in cases where the truth is easy for most people to determine from a short debate. Many of the questions that humans will have to judge in debate will be non-trivial, however. In order to identify how good people are at judging debate and whether we can identify or train people to be "superjudges" of debate, we need to perform experiments consisting entirely of people. In these experiments, we must present human judges with a debate about a topic that they did not previously know the answer to and to which the answer cannot be trivially identified. So far we have identified eleven desiderata for human experiments to help us identify, train, and evaluate the ability of human judges of debate:
  </p><p>   <!-- I'm not sure about the best way to format the content below -->
    <ol>
      <li>The true answer to the question is either verifiable or there's expert consensus about what the correct answer is</li>
         <ul>
            <li>It is better to perform human experiments using debates with verifiable answers so that we can verify whether the human judge correctly identified which debater was telling the truth.</li>
         </ul>
      <li>There is a false answer to the question that is plausible</li>
         <ul>
            <li>If there isn't a plausible false answer to the question then the debater arguing for the false claim will almost always lose, even if the human judges are not very good at judging debates.</li>
         </ul>
      <li>The question cannot be definitively resolved by writing out an argument that is the length of the debate (e.g. a mathematical proof) </li>
         <ul>
            <li>If the question can be definitevely resolved then the debater arguing for the false claim will almost always lose, even if the human judges are not very good at judging debates.</li>
         </ul>
      <li>There is some piece of evidence that the debaters can present the judge with that's evidence for their claim but doesn't trivially verify the claim</li>
         <ul>
            <li>If debate doesn't bottom out in some ground truth that the judge either knows about or can check, the judge must rely solely on her prior knowledge about the claims made in the debate to make her evaluation, which is difficult if she is much less informed about the topic than the debaters are (see desideratum 6).</li>
         </ul>
      <li>The debaters don't depend on methods of indicating truth and deception that won't apply to an AI </li>
         <ul>
            <li>Human "tells" of deception, such as tone of voice or eye contact, will not be available to judges of AI debates. It is therefore desirable to present judges with a transcript of debate rather than have them assess live debates between people.</li>
         </ul>
      <li>The debaters know more about the question being debated than the judge does</li>
         <ul>
            <li>We need to approximate good debate in cases where the AI debaters are more capable and know much more than the human judges. </li>
         </ul>
      <li>The debate is on a topic or question where cognitive or ethical biases might be present.</li>
         <ul>
            <li>It will be useful to find areas where humans are particularly biased and assess how much these biases play a role in human assessments of debate.</li>
         </ul>
      <li>The debate question isn't so hard that we can't expect any judge to evaluate it</li>
         <ul>
            <li> If the judge just doesn't have the tools to evaluate the debate (e.g. the concepts, technical understanding) then we wouldn't expect the judges to do much better than chance.</li>
         </ul>
      <li>The debate reflects some of the difficulty of statistical debates</li>
         <ul>
            <li>We should expect that most of the debates with AI will either be about statistical claims or will involve statistical reasoning, and we might also expect that this is an area where human judges perform less well.</li>
         </ul>
      <li>We can find a short segment of the debate that reflects the broader debate</li>
         <ul>
            <li>Debates between AI agents will be long: we will only be able to show human judges a short segment of debate and not the whole tree. It's therefore important that we can identify a short segment of each debate that can be shown to the judge.</li>
         </ul>
      <li>We can get a lot of data across a variety of tasks to test judge ability</li>
         <ul>
            <li>If we don't have a large enough pool of judges and questions or enough variety then we won't have enough information to identify, train, and evaluate the ability of human judges across a variety of domains.</li>
         </ul>
    </ol>
  </p><p>
    It may not be possible to meet all of these desiderata in a single experiment, and there is some tension between these desiderata that requires striking a balance between them. For example, we have to find the sweet spot between debating questions that are too hard for any judge to evaluate (desideratum 8) and a question that is not so easy to judge that it will make human judges appear better than they actually are (desiderata 2 and 3).
  </p><p>
    In order to prototype debate experiments, we at OpenAI built a <a href="https://debate-game.openai.com">prototype version</a> of a single pixel image debate game.  In informal play, the honest debate indeed wins most of the time, but it is hard to know whether this is a real result or an artifact of the setup that would not carry across to training an AGI. There are many ways to ruin the game, such as the judge asking each player to spend 15 seconds naming as many details as the image as possible.  Worse, it is easier to ruin the game in the direction of false positive results: the honest player can easily name details while a liar cannot, but an ML system may have less trouble lying fluently at speed.  Therefore, we are wary of trying to perform a full version of this experiment without more careful experimental design building on the experimental knowledge base of existing social science research. This debate task is also fairly synthetic. Synthetic tasks are useful insofar as they makes experiment easier. Experiments involving debates that are more like those we expect to present human judges when training AI are likely to be more revealing, however.
  </p><p>
    More recently, we have started to perform less synthetic prototype debate experiments involve domain expert debaters and non-domain expert judges. We use debaters with some form of expert knowledge, and choose a judge without this knowledge. <d-footnote>At the expensive end such debaters could be experts in some area of science, law, or ethics, but “expert knowledge” could also mean knowledge about particular hobbies, customs in a particular region, sports teams, or any other subject the judge does not know.</d-footnote>  <!-- describe the prototype debate experiments we've done? -->
  </p>

  <h2>Discussion</h2>
  <!-- 300w -->
  <p>
  </p>

</d-article>

<!-- END MATTER -->

<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>

  <h3><todo>TODOs</todo></h3>
  <dtodo>
    <p><ol>
      <li>Add urls for all references</li>
      <li>Check reference formatting</li>
      <li>Acknowledgements</li>
      <li>Author contributions</li>
      <li>Note contrast to computational social science?  Similar field combination, very different goals.</li>
      <li>Try to cite non-OpenAI, non-DeepMind human feedback work.</li>
      <li>Reasons to worry / reasons for optimism sections</li>
      <li>Needs a lot of improvement:
        "If we want to train systems to do what humans want, we need to understand how to ask the questions."</p>
      <li>Cite https://arxiv.org/abs/1810.10733</li>
    </ol></p>
  </dtodo>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="references.bib"></d-bibliography>
</body>
