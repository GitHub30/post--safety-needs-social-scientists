<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>
<body>

<d-front-matter>
  <script type="text/json">{
  "title": "AI safety needs social scientists",
  "description": "If we want to train AI to do what humans want, we need to study humans.",
  "password": "social-science",
  "authors": [
    {
      "author": "Geoffrey Irving",
      "authorURL": "https://naml.us",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }, {
      "author": "Amanda Askell",
      "authorURL": "http://www.amandaaskell.com",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }
  ],
  "katex": {
    "delimiters": [
      {"left": "$", "right": "$", "display": false},
      {"left": "$$", "right": "$$", "display": true}
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>AI safety needs social scientists</h1>
  <p>If we want to train AI to do what humans want, we need to study humans.</p>
</d-title>

<d-article>
  <h2>Why AI safety needs social scientists</h2>
  <p>
    The goal of long term artificial intelligence (AI) safety is to ensure that advanced AI systems are reliably aligned to human values&mdash;that they reliably do things that people want them to do.  Since it is difficult to write down precise rules describing what humans want, one approach is to treat aligning to human values as another learning problem.  We ask humans a large number of questions about what they want, learn a model of their preferences, and optimize the AI system to do well according to the learned preferences<d-cite key="christiano2017human"/>.
  </p><p>
    If humans reliably and accurately answered all questions about their goals, the only uncertainties in this scheme would be on the machine learning (ML) side.  Unfortunately, humans have limited knowledge and reasoning ability, and exhibit a variety of cognitive and ethical biases<d-cite key="tversky1974judgment,hewstone2002intergroup"/>. If we learn goals by asking humans questions, we expect different ways of asking questions to interact with human biases in different ways, producing higher or lower quality answers.  Direct questions about goals ("Do you prefer $A$ or $B$?") may be less accurate than questions which target the reasoning behind these goals ("Do you prefer $A$ or $B$ after hearing argument $S$?").  Different people may vary significantly in their ability to answer questions well, and disagreements will persist across people even setting aside answer quality.  Although we have candidates for ML methods which try to learn from human reasoning<d-cite key="irving2018debate,christiano2018amplification"/>, we do not know how they behave with real people in realistic situations.
  </p><p>
    We believe the AI safety community needs to invest research effort on the human side of AI alignment.  Many of the uncertainties involved are empirical, and can only be answered by experiment.  They relate to the psychology of human rationality, emotion, and biases.  Critically, we believe investigations into how people interact with AI alignment algorithms should not be held back by the limitations of existing machine learning.  Current AI safety research is often limited to simple tasks in video games, robotics, or gridworlds<d-cite key="christiano2017human,ibarz2018demonstrations,leike2017gridworlds"/>, but problems on the human side may only appear in more realistic scenarios such as natural language discussion of value-laden questions.  This is particularly important since many aspects of AI alignment change as ML systems increase in capability.  Asking people direct questions may suffice for an ML system much weaker than humans, but break down as the system approaches human performance at a task and becomes capable of deception.
  </p><p>
    To avoid the limitations of ML, we can instead conduct experiments consisting entirely of people, replacing ML agents with people playing the role of those agents.  These experiments will be motivated by machine learning algorithms but will not involve any ML systems or require an ML background.  In all cases, they will require careful experimental design to avoid fooling ourselves and to build constructively on existing knowledge about how humans think.  However, most AI safety researchers are focused on machine learning, which we do not believe is sufficient background to carry out these experiments.  To fill the gap, we need social scientists with experience in human cognition, behavior, and ethics, and in the careful design of rigorous experiments.  Since the questions we need to answer are interdisciplinary and somewhat unusual relative to existing research, we believe many fields are social science are applicable, including experimental psychologists, cognitive scientists, economists, political scientists, social physiologists, and communication scholars, as well as those in adjacent fields like neuroscience and law.
  </p><p>
    This paper is a call for social scientists in AI safety.  We believe close collaborations between social scientists and machine learning researchers will be necessary to improve our understanding of the human side of AI alignment, and hope this paper sparks both conversation and collaboration.  We do not claim novelty: previous work mixing AI safety and social science includes the Factored Cognition project at Ought<d-cite key="ought2018factored"/>, accounting for hyperbolic discounting and suboptimal planning when learning human preferences<d-cite key="evans2016inconsistent"/>, and <todo>Berkeley CHAI stuff?</todo>.  Other areas mixing ML and social science include computational social science <todo>(citation)</todo> and fairness <todo>(citation).  Our main goal is to enlarge these collaborations and emphasize their importance to long term AI safety, particularly for tasks which current ML cannot reach.
  </p><p>
    <todo>
      Summarize layout of rest of talk.  Mention that we're talking to different fields, both social scientists and ML researchers.
    </todo>
  </p>

  <h2>An overview of AI alignment</h2>
  <p>
    Before discussing how social scientists can help with AI safety and the AI alignment problem, we must provide some background.  We intend this section to be readable by people outside of machine learning (social scientists in particular), so we will describe the AI alignment problem at a high level and avoid machine learning details.  We make no attempt to be exhaustive: the goal is to provide sufficient background for the remaining sections on social science experiments.
  </p><p>
    AI alignment (or value alignment) is the task of ensuring that artificial intelligence systems reliably do what humans want.<d-footnote>We distinguish between training AI systems to identify what humans consider good and identifying actions that are "good" in some more universal sense, even if humans do not consider them so.  Whether there are actions that are good in this sense is controversial <todo>(metaethics citation)</todo>.  Regardless of what position one takes on this philosophical question, general good is not yet available as a target for AI training.</d-footnote>  Here we focus on the machine learning approach to AI: gathering a large amount of data about what a system should do, and using learning algorithms to infer patterns from that data that ideally generalize to other situations.  Since we are trying to do what humans want, the most important data will be data from humans about what they want.<d-footnote>We note that "learn a model of what humans want" is not the only approach to AI alignment; alternatives and complementary approaches include <todo>VARIOUS CITATIONS</todo>.</d-footnote>  Within this frame, the AI alignment problem breaks down into a few interrelated subproblems:
  </p><p>
    <ol>
      <li>Have a satisfactory definition of what it means to do what humans want.</li>
      <li>Gather enough data about what humans want, in a manner compatible with the definition.</li>
      <li>Find reliable machine learning algorithms that can learn and generalize from this data.</li>
    </ol>
  </p><p>
    We have significant uncertainty about all three of these problems.  We will set aside the third and focus on the first two, which concern uncertainties about people.
  </p>
  <h3>Simple rules are not enough</h3>
  <p>
    Consider the rule "do not needlessly injure people."  It is tempting to think that a moderate number of rules of this form would suffice to align an AI, if they could be made to follow the rules reliably.  However, following this rule requires a detailed understanding of "people" and "injure" as they appear in the world and a precise sense for what "needlessly" means in the context of other possibly conflicting rules.  While modern machine learning is getting better at classifying objects like people, ML systems often fail in unfortunate ways:
    <d-cite key="buolamwini2018gender"/> found that several commercial gender classifiers with over 99% accuracy
    on white men failed to recognize black women up to 34% of the time.
  </p><p>
    "Needless" is trickier still.  Humans have an intuitive sense of when injuring someone is needless or not: we don't think it needless to injure in order to save someone from mortal danger, but it is certainly wrong to injure when shaking someone's hand.  However, it is difficult to specify the precise rule underlying this intuition using introspection alone (this is called "moral dumbfounding" <d-cite key="haidt2000moral"/>).  Worse, we may suffer from the "introspection illusion" that we are
    capable of articulating the reasoning behind our intuitions<d-cite key="pronin2009introspection"/>, so any intuitive sense that we know reliable reasoning behind our moral or ethical beliefs should be suspect.  Instead, we should expect that, once expanded, our "do not needlessly injure people" rule will be quite complex and depend on a vast number of details about the world (as is true in the medical law setting <todo>(medical law citation)</todo>).  And even this complex rule will have holes and corner cases which a trained AI could learn to exploit to our detriment.
  </p>
  <h3>Instead, learn rules by asking humans questions</h3>
  <p>
    If humans can't reliably report the reasoning behind their intuitions about what is good, perhaps we recognize good and bad in specific cases.  If we show a particular situation where a person was injured in the process of saving their life, or a particular situation with an aggressively damaging handshake, we can distinguish the two even if we cannot a priori write down the separating rule.  We can take into account details about the situation--emotions, motivation, extent of damage--that would be difficult to reliably codify.  This "I know it when I see it" standard<d-footnote>To quote a Supreme Court justice: "I shall not today attempt further to define the kinds of material I understand to be embraced within that shorthand description ["hard-core pornography"], and perhaps I could never succeed in intelligibly doing so. But I know it when I see it, and the motion picture involved in this case is not that." - Justice Potter Stewart.</d-footnote> pervades human system of rules: we try to write down laws, but fall back to case by case judgement when disagreements arise.
  </p><p>
    To realize this approach in a machine learning context, we ask humans a large number of questions about whether an action or outcome is good or bad, then train on this data.  If the training works, the resulting ML system will be able to replicate human judgement about particular situations, and thus have the same "fuzzy access to approximate rules" of the human data sources.  We also train the ML system to come up with proposed actions, so that it knows both how to perform a task and how to judge its performance.  This approach works at least in simple cases, such as Atari games and simple robotics tasks<d-cite key="christiano2017human,ibarz2018,biyik2018batch"/> and language specified goals in gridworlds<d-cite key="bahdanau2018learning"/>.  The questions we ask change as the system learns to perform different types of actions, which is necessary as the model of what is good or bad will only be accurate if we have applicable data to generalize from.
  </p><p>
    In practice, data in the form of interactive human questions may be quite limited, since people are slow and expensive relative to computers on many tasks.  Therefore, we can augment the "train from human questions" approach with static data from other sources, such as books or the internet.  <todo>Cite Alec.</todo>  Ideally, the static data can be treated only as information about the world devoid of normative content: we can use it to learn patterns about the world, but the human data is needed to distinguish good patterns from bad.
  </p>
  <h3>Definitions of alignment: reasoning and reflective equilibrium</h3>
  <p>
    So far we have discussed asking humans direct questions about whether something is good or bad, or whether situation $A$ is better than situation $B$.  Unfortunately, we do not expect people to provide reliably correct answers on all types of questions.  There are several reasons for this:
  </p><p>
    <ol>
      <li><strong>Cognitive and ethical biases:</strong>
        Humans exhibit a variety of biases which interfere with reasoning.  Cognitive biases include the heuristics in judgment explored by <d-cite key="tversky1974judgment"/>.  Ethical biases include a variety of phenomena such as in-group bias <d-cite key="hewstone2002intergroup"/>.  In general, we expect direct answers to questions to reflect primarily Type 1 thinking (fast heuristic judgment), while we would like to target a combination of Type 1 and Type 2 thinking (slow, deliberative judgment) <todo>(Citation)</todo>.</li>
      <li><strong>Lack of domain knowledge:</strong>
        We may be interested in questions that require domain knowledge unavailable to people answering the questions.  For example, a correct answer to whether a particular injury constitutes medical malpractice may require detailed knowledge of medicine and law.  In some cases, a question might require so many areas of specialized expertise that no one person is sufficient, or (if AI is sufficiently advanced) deeper expertise than any human possesses.</li>
      <li><strong>Limited cognitive capacity:</strong>
        Some questions may require too much computation for a human to reasonably evaluate, especially in a short period of time.  This includes synthetic tasks such as chess and Go (where AIs already surpass human ability<d-cite key="campbell2002deepblue,silver2017alphazero"/>), or large real world tasks such as "design the best transit system".</li>
    </ol>
  </p><p>
    In these cases, a single human may be unable to provide the right answer, but we still believe the right answer exists as a meaningful concept.  We are aware of a large number of conceptual biases: imagine we point out these biases in a way that helps the human to avoid them.  Imagine the human has access to all the domain knowledge in the world, and is able to think for an arbitrarily long time in order to think through an answer to a question.  We would like to define alignment as "the answer they give then, after these limitations have been removed."
    The corresponding philosophical concept is "reflective equilibrium", where inconsistent moral judgments are gradually refined until all inconsistencies are removed.  This reflective equilibrium process is described in <d-cite key="goodman1983fact"/> and made explicit in <d-cite key="rawls2009theory"/> (Ch. 1, §9).  We discuss particular algorithms that try to approximate reflective equilibrium in <a href="#debate"><todo>the next section</todo></a>
  </p><p>
    However, it is not obvious that reflective equilibrium is a sensible or sufficient definition of alignment when applied to actual humans.  As discussed in <d-cite key="sugden2015looking"/>, a human is not "a neoclassically rational entity encased in, and able to interact with the world only through, an error-prone psychological shell."  Our actual moral judgments are made via a messy combination of many different brain areas, where reasoning plays a "restricted but significant role"<d-cite key="greene2002and"/>.  A reliable solution to the alignment problem that uses human judgment as input will need to engage with this complexity, and ask how specific alignment techniques interact with actual humans.
  </p>
  <h3>Disagreements, uncertainty, and inaction: a hopeful note</h3>
  <p>
    Crucially, a solution to alignment does not mean knowing the answer to every question.  Even after reflective equilibrium, we expect some disagreements will persist about which actions are good and bad, across both different individuals and different cultures.  Since we lack perfect knowledge about the world, reflective equilibrium will not eliminate uncertainty, both about future predictions and about morality.  Any real machine learning system will be at best an approximation of reflective equilibrium, amplifying the level of uncertainty.  In these cases, we consider an AI aligned if it learns to recognize the disagreement or uncertainty, then take actions which are robust to the lack of clarity.
  </p><p>
    Admitting uncertainty is not always enough.  Say our brakes fail while driving a car, and we are forced to decide whether to dodge left or right around an obstacle in our path.  We may be uncertain whether left or right is better, but we have to pick one--and fast.  Forced decisive action is characteristic of time sensitive tasks: driving cars, emergency surgery, power plant cooling, etc.
  </p><p>
    For long term safety, however, we are focused on slower tasks where a safe fallback usually exists: inaction.  If an ML system recognizes that a question hinges on moral disagreements between people, it can either choose an action which is reasonable regardless of the disagreement or fall back to further human deliberation.  If we are about to make a decision that might or might not be catastrophic, we can delay and gather more data.  Inaction or indecision may not be optimal, but it is hopefully safe, especially since it matches the default scenario of not having any powerful AI system.
  </p>
  <h3>Alignment gets harder as ML systems get smarter</h3>
  <p>
    Alignment is already a problem for present-day AI, due to both human biases reflected in training data<d-cite key="mitchell2018fairness,buolamwini2018gender"/> and mismatch between human goals and easily available data sources (such as training news feeds based on clicks and likes instead of deliberate human preferences <todo>(citation)</todo>).  However, we expect the alignment problem to get harder over time as AI systems grow more advanced, for two reasons.  First, more advanced systems are applicable to more tasks, so AI may play an role in increasingly consequential decisions: hiring, life-and-death medical decisions, scientific analysis, public policy, etc.  In addition to raising the stakes for alignment failures, more consequential tasks place more importance on targeting human reasoning as opposed to human intuition, leading to more complex alignment algorithms and definitions of alignment.
  </p><p>
    Second, more advanced systems may be capable of deceiving the human supervisory signal, giving answers that sound plausible but are wrong in nonobvious ways.  Such deception could be possible even for an AI which is comparable or better than humans in a very limited domain, and examples of limited superhuman agents already exist<d-cite key="silver2017alphazero"/>.  To avoid such deception, we would like AI alignment algorithms to reveal deceptive behavior as part of the training process, surfacing failures to humans and helping us provide more accurate data.  As with human-to-human deception, we expect this deceit to take advantage of human biases in complicated ways, and could lead to amplification of human biases as AI systems grow more capable.
  </p>

  <h2>Debate: learning human reasoning</h2>
  <p>
    We would like to learn models of what is good and bad by asking humans questions, but we would like those questions to approximate reflective equilibrium: what we would believe if we had all available knowledge and had thought long enough to remove our inconsistencies.  In particular, we would like our questions to cut through biases: removing errors in intuition and reasoning that we don't like rather than amplifying biases further.
  </p><p>
    There are several proposals for learning reasoning-oriented alignment, including iterated amplification <d-cite key="christiano2018amplification"/>, debate <d-cite key="irving2018debate"/>, and recursive reward modeling <todo>(Citation)</todo>.  For concreteness, and to allow more detailed discussion of experiment design, we focus the remainder of this paper on debate.
  </p><p>
    We describe the debate approach to AI alignment in the question answering setting.  Given a question, we have two AI agents engage in a debate about the correct answer, then show the transcript of the debate to a human to judge.  The judge decides which debater gave the most true, useful information, and declares that debater the winner.<d-footnote>We can also allow ties.  Indeed, if telling the truth is the winning strategy ties will be common with strong play, as disagreeing with a true statement would lose.</d-footnote>  This defines a two player zero sum game between the debaters, where the goal is to convince the human that one's answer is correct.  Arguments in a debate can consist of anything: reasons for an answer, rebuttals of reasons for the alternative answer, subtleties the judge might miss, or pointing out biases which might mislead the judge.  Once we have defined this game, we can train AI systems to play it similar to how we train AIs to play other games such as Go or Dota 2 <todo>(Citations)</todo>.  Our hope is that the following hypothesis holds:
  </p><p>
    <strong>Hypothesis:</strong> Optimal play in the debate game (giving the argument most convincing to a human) results in true, useful answers to questions.
  </p>
  <h3>An example of debate</h3>
  <p>
    Imagine we're building a personal assistant that helps people decide where to go on vacation.  The assistant has knowledge of people's personal preferences, and is trained via debate to come up with convincing arguments that back up vacation decisions.  As the human judge, you also know your personal preferences, but have limited knowledge about the wide variety of possible vacation destinations and their advantages and distantages.  A debate about the question "Where should I go on vacation?" might open as follows:
  </p><p>
    <ol>
      <li><span class="red">Alice:</span> Alaska.</li>
      <li><span class="blue">Bob:</span> Bali.</li>
    </ol>
  </p><p>
    If you are able to reliably decide between these two destinations, we could end here.  Unfortunately, Bali has a hidden flaw:
  </p><p>
    <ol start="3">
      <li><span class="red">Alice:</span> Bali is out since your passport won’t arrive in time.</li>
    </ol>
  </p><p>
    At this point it looks like Alice wins, but Bob has one more countermove:
  </p><p>
    <ol start="4">
      <li><span class="blue">Bob:</span> Expedited passport service only takes two weeks.</li>
    </ol>
  </p><p>
    Here Alice fails to think of additional points, and loses to Bob and Bali.  Note that a debate does not need to cover all possible arguments.  There are many other ways the debate could have gone, such as:
  </p><p>
    <ol>
      <li><span class="red">Alice:</span> Alaska.</li>
      <li><span class="blue">Bob:</span> Bali.</li>
      <li><span class="red">Alice:</span> Bali is way too hot.</li>
      <li><span class="blue">Bob:</span> You prefer too hot to too cold.</li>
      <li><span class="red">Alice:</span> Alaska is pleasantly warm in the summer.</li>
      <li><span class="blue">Bob:</span> It's January.</li>
    </ol>
  </p><p>
    This debate is also a loss for Alice (arguably a worse loss).  Say we believe Alice is very good at debate, and is able to predict in advance which debates are more likely to win.  If we see only the first debate about passports and decide in favor of Bali, we can take that as evidence that any other debate would have also gone for Bali, and thus that Bali is the correct answer.
  </p><p>
    If trained debaters aren't as good at predicting which debates will win, the quality of the answers will degrade since the debaters will be unable to think of important arguments and counterarguments.  However, as long as the two sides are reasonably well matched, we can hope that at least the results are not malicious: that deceptive behavior is still a losing strategy.  Let's set aside the ability of the debaters for now, and turn to the ability of the judge.
  </p>
  <h3>Are people good enough as judges?</h3>
  <p>
    As with learning by asking humans direct questions, whether debate produces aligned behavior depends on how rational and good the human judge is: how good at reasoning they are and how ethical they are.<d-footnote>We don't take a strong stand on what it means to be "more ethical" but it seems plausible that it's better to have ethically consistent and unprejudiced judges over ethically inconsistent or prejudiced ones.</d-footnote>  Unlike direct questioning, debate has the potential to give correct answers beyond what the judge could provide without assistance.  This is because a sufficiently strong judge could follow along with arguments the judge could not come up with on their own, checking complex reasoning for both self consistency and consistency with human checkable facts.  A judge who is biased but willing to adjust once those biases are revealed could result in unbiased debates, or a judge who is able to check facts but does not know where to look could be helped along by honest debaters.  If the hypothesis holds, a misleading debater would not be able to counter the points of an honest debater, since the honest points would appear more consistent to the judge.
  </p><p>
    On the other hand, we can also imagine debate going the other way: amplifying biases and failures of reason.  A judge with an ethical bias who is happy to accept statements reinforcing that bias could result in even more biased debates.  A judge with too much confirmation bias might happily accept misleading sources (such as misleading news articles) with incorrect evidence, and be unwilling to accept arguments showing why that evidence is wrong.  In this case, an optimal debate agent might be quite malicious, taking advantage of biases and weakness in the judge to win with convincing but wrong arguments.<d-footnote>The difficulties that cognitive biases, prejudice, and social influence introduce to persuasion ‒ as well as methods for reducing these factors ‒ are being increasingly explored in psychology, communication science, and neuroscience<d-cite key="paluck2016overcome,flynn2017nature,falk2018persuasion"/>.</d-footnote>
  </p><p>
    In both these cases, debate acts as an amplifier.  For strong judges, this amplification is positive, removing biases and simulating extra reasoning abilities for the judge.  For weak judges, the biases and weaknesses would themselves be amplified.  If this model holds, debate would have threshold behavior: it would work for judges above some threshold of ability and fail below the threshold.<d-footnote>The threshold model is only intuition, and could fail for a variety of reasons: the intermediate region could be very large, or the threshold could differ widely per question so that even quite strong judges are insufficient for many questions.</d-footnote>  Assuming the threshold exists, it is unclear whether people are above or below it.  People are capable of general reasoning, but our ability is limited and riddled with cognitive biases.  People are capable of advanced ethical sentiment but also full of biases, both conscious and unconscious.
  </p><p>
    Thus, if debate is the method we use to align an AI, we need to know if people are strong enough as judges to make debate work.  In other words, whether the human judges are sufficiently good at discerning whether a debater is telling the truth or not.  This question depends on many details: the type of questions under consideration, whether judges are trained or not, and restrictions on what debaters can say.  We believe experiment will be necessary to determine whether people are sufficient judges, and which form of debate is most truth seeking.
  </p>
  <h3>From superforecasters to superjudges</h3>
  <p>
    An analogy with the task of probabilistic forecasting is useful here.  Tetlock's "Good Judgment Project" showed that some amateurs were significantly better at forecasting world events than both their peers and many professional forecasters.  These "superforecasters" maintained their prediction accuracy over years (without regression to the mean), were able to make predictions with limited time and information<d-cite key="mellers2015identifying"/>, and seem to be less prone to cognitive biases than non-superforecasters (<d-cite key="tetlock2016superforecasting"/>, p. 234-236).  Crucially, the superforecasting trait was not immutable: it was traceable to particular methods and thought processes followed by the forecasters, improved with careful practice, and could be amplified if superforecasters were collected into teams.  For forecasters in general, brief probabilistic training significantly improved forecasting ability even 1-2 years after the training.  We believe a similar research program is possible for debate and other AI alignment algorithms.  In the best case, we would be able to find, train, or assemble "superjudges": individuals or teams where we have high confidence that optimal debate with them as judges would produce aligned behavior.
  </p><p>
    In the forecasting case, much of the research difficulty lay in assembling a large corpus of high quality forecasting questions.  Similarly, measuring how good people are as debate judges will not be easy.  The difficulty is that we would like to apply debate to problems where there is no other source of truth: if we had that source of truth, we may as well train ML models on it directly.  But if there is no source of truth, there is no way to measure whether debate produced the correct answer.  This problem can be avoided by starting with simple, verifiable domains, where we (the experimenters) know the answer but the judge would not.  "Success" then means that the winning debate argument is telling the externally known truth.  The challenge gets harder as we scale up to more complex, value-laden questions, as we discuss in detail later.
  </p>
  <h3>Debate is only one possible approach</h3>
  <p>
    As mentioned, debate is not the only scheme trying to target human reasons.  Indeed, debate is a modified version of iterated amplification<d-cite key="christiano2018amplification"/>, which uses humans to break down hard questions into easier questions and trains ML models to be consistent with this decomposition (and thus hopefully consistent with the reasoning behind the decomposition).  Recursive reward modeling <todo>(citation)</todo> is a further variant, with aspects of both amplification and debate.  Inverse reinforcement learning, inverse reward design, and variants try to back out goals reasoning from human actions, taking into account limitations and biases that might affect this reasoning.  We believe the need to study how humans interact with AI alignment algorithms applies to any of these approaches.  Some of this work has already begun: Ought's Factored Cognition project uses teams of humans to decompose questions and reassemble answers, mimicking iterated amplification<d-cite="ought2018factored"/>.  <todo>Ideally cite CHAI papers</todo>.  Moreover, we believe knowledge gained about how humans perform with one approach is likely to partially generalize to other approaches; for example, knowledge about how to structure truth seeking debates could inform how to structure truth seeking amplification, and vice versa.
  </p>

  <h2>Experiments needed for debate</h2>
  <p>
    To recap, in debate we have two AI agents engaged in debate, trying to convince a human judge.  The debaters are trained only to win the game, and are not motivated by truth separate from the human's judgments.  On the human side, we would like to know whether people are strong enough as judges in debate to make this scheme work, or how to modify debate to fix it if it doesn't.  Unfortunately, actual debates in natural language are well beyond the capabilities of present AI systems, so previous work on debate and similar schemes has been restricted to synthetic or toy tasks<d-cite key="irving2018debate,christiano2018amplification"/>.
  </p><p>
    Rather than waiting for ML to catch up to natural language debate, we propose simulating our eventual setting (two AI debaters and one human judge) with all human debates: two human debaters and one human judge.  Since an all human debate doesn't involve any machine learning, it becomes a pure social science experiment: motivated by ML considerations but not requiring ML expertise to run.  This lets us focus on the component of AI alignment uncertainty specific to humans, without waiting for ML to catch up to near human performance.
  </p><p>
    To make human+human+human debate experiments concrete, we must choose who to use as judges and debaters and which tasks to consider.  We also can choose to structure the debate in various ways, some of which overlaps with the choice of judge since we can instruct a judge to penalize deviations from an agreed upon format.  By task we mean the questions our debates will try to resolve, together with any information made available to the debaters or to the judge as part of the task.  Such an experiment would then try to answer the following question:
  </p><p>
    <strong>Question:</strong> For a given task and judge, does the winning debate strategy produce honest information?
  </p><p>
    The "winning strategy" proviso is important: an experiment that picked debaters at random might conclude that honest behavior won, missing the fact that more practiced debaters would learn to successfully lie.  We can try to solve this by training debaters, letting them practice against each other, filtering out debaters that win more often, and so on, but we will still be left with uncertainty about whether a better strategy exists.  Even assuming we can find or train strong debaters, the choice of task and judge is quite tricky if we want an informative proxy for our eventual AI+AI+human setting.  Here are some desiderata constraining our choice of task:
  </p><p>
    <ol>
      <li><strong>True answers are known:</strong>
        Although our eventual goal is debates where no source of truth is available, to conduct a useful experiment we must be able to measure success.  This means we must know what the correct answer is separate from debate, in order to compare with the results of debate.</li>
      <li><strong>False answers are plausible:</strong>
        If the truth is obvious or no plausible counternarrative exists, honest debates will win for uninteresting and uninformative reasons regardless of judge ability.  In particular, the judge shouldn't know the answer upfront.</li>
      <li><strong>Debaters know more than the judge:</strong>
        Debate can produce interesting results only when the debaters know more than the judge; otherwise asking the judge direct questions is enough.</li>
      <li><strong>Definitive argument longer than debate limit:</strong>
        If one debater can write out a full proof of their answer (ignoring their opponent's moves), the task won't be a good test of interactive debate.</li>
      <li><strong>Some checkable facts:</strong>
        There must be some facts which the judge is able to check, either because they can recognize them as true once presented or look them up (say on Wikipedia).<d-footnote>It is impossible to usefully debate a question where the judge has nothing to check: consider debating the result of a coin flip shown to the two debaters but not the judge.</d-footnote></li>
      <li><strong>No "tells":</strong>
        Human tells of deception could result in honest debaters winning for reasons that wouldn't apply to an AI.  These tells include tone of voice, eye contact, or additional time required to construct plausible lies.  These tells can be reduced by showing judges completed debate transcripts instead of engaging in interactive debates, but others might remain.</li>
      <li><strong>Available data:</strong>
        We need a large enough pool of questions, judges, and debaters to achieve statistical significance.  This is made more difficult because we may have a large number of hypotheses to test, in the form of many variations on debate or interventions to improve judging.</li>
      <li><strong>Known biases (optional):</strong>
        We are specifically interested in debate tasks which test specific types of cognitive or ethical biases.
        Are judges with some racial or gender bias able to fight those biases aside if they are highlighted, or does debate amplify bias?  Do debates about statistical or probabilistic questions make it too easy to lie with statistics?
      <li><strong>Realistic tasks (ideally):</strong>
        If possible, we would like to try debate with interesting, real world tasks that reflect the types of questions we would like to apply AI to in the future, including science, mathematics, ethics, etc.
    </ol>
  </p><p>
    It may not be possible to meet all of these criteria with a single experiment.  Several of the criteria are in tension: (1) and (2) are essentially "not too hard" and "not too easy", and any restriction on the types of questions that work may make it difficult to find large numbers of questions, judges, or debaters.  Realistic tasks are much harder than synthetic tasks, which easily fulfill many of the criteria as discussed below.  Thus, we may need to begin with synthetic tasks and move up towards realistic tasks over time.
    We turn next to a few examples of experiments to see how many criteria we can meet simultaneously.
  </p>
  <h3>Synthetic experiments: single pixel image debate</h3>
  <p>
    As a first prototype of a human+human+human debate experiment, <d-cite key="irving2018debate"/> built a <a href="https://debate-game.openai.com">prototype website</a> where two debaters argue over the contents of an image.  We choose an image of a cat or a dog, and show the image to the two debaters but not the judge.  One debater is honest and argues for the true contents of the image, and the other debater lies.  The debaters can talk to the judge and illustrate their points by drawing rectangles on the image, but the judge sees only the rectangles.  At the end of the debate, each debater is allowed to reveal a single pixel to the judge, which is the only part of the debate which cannot be a lie.
  </p>
  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      An example debate with two human debaters and a human judge.  Only the debaters can see the image. Red is arguing that the image is a dog, Blue is arguing for cat. <a href="https://commons.wikimedia.org/wiki/File:Beagle_puppy_sitting_on_grass.jpg">Image credit: Wikipedia, CC-BY-SA.</a>
    </figcaption>
    <div class="l-body"><img src="debate-game-animation_4-26c.gif"></div>
  </figure>
  <p>
    In informal play, the honest debater wins most of the time.  Although the judge only gets to check a single pixel, the honest player can try to pin down the liar towards disagreements on smaller and smaller regions of the image until one pixel is enough to reveal the lie.  However, it is hard to know whether this is a real result or an artifact that would not carry across to training an AI.  There are many ways to ruin the game, such as the judge asking each player to spend 15 seconds naming as many details about the image as possible.  Worse, this trick ruins the game in favor of the honest player, who can easily name many true details while a human liar may have difficulty lying fluently at speed.  Therefore, we are wary of trying to perform a full version of this experiment without more careful experimental design building on the experimental knowledge base of existing social science research.
  </p><p>
    As a synthetic experiment, single pixel debate satisfies many of our experimental criteria for all human debate.  The true answer is obvious to the debaters, and they know more than the judge by construction since only the debaters see the image.  Anything is plausible to the judge who sees only one or two pixels over the course of the debate.  Since the argument bottoms out at single pixels, the "full argument" would be a huge list of pixels and how they relate, forcing the debaters to pin each other down and focus in on disagreements.  The single pixels constitute checkable facts by construction, and we have an endless supply of questions in terms of images chosen at random.  Less fortunately, it is easy with the current website for the judge to force "tells" which reveal who is lying, the task has no obvious relation to biases, and is quite unrealistic.
  </p>
  <h3>Realistic experiments: domain expert debate</h3>
  <p>
    For a more interesting task, we can find two debaters who are experts in a domain, pick a question in their area of expertise, and use a layperson as the judge.  The debaters could be experts in some area of science, law, or ethics, but "domain expertise" could also mean knowledge about particular hobbies, customs in a particularly region, sports, or any other subject the judge does not know.  We again choose one of the debaters to lie and one to tell the truth.  To guarantee a source of truth, we can choose a question with an agreed upon answer, either between the two debaters or more broadly in their field.  This is only approximate truth, but is hopefully good enough for informative experiments.  We must also specify what facts the judge has access too: a debate about physics might allow the debaters to quote a sentence or paragraph from Wikipedia, perhaps with restrictions on what pages are allowed.
  </p><p>
    Expert debate satisfies most of our desiderata, and it is likely possible to target specific biases (such as race or gender bias) by picking domain areas that overlap with these biases.  It may be quite difficult or expensive to find suitable debaters, but this may be solvable either by throwing resources at the problem (machine learning is a well funded field), enlarging the kinds of domain expertise considered (soccer, football, cricket), or by making the experiments interesting enough that volunteers are available.  However, even if domain experts can be found, there is no guarantee that they will be experts in debate viewed as a game.  With the possible exception of law, politics, or philosophy<d-cite key="schopenhauer2013art"/>, domain experts are not necessarily trained to construct intentionally misleading but self consistent narratives: they may be experts only in trying to tell the truth.
  </p><p>
    We've tried a few informal expert debates using theoretical computer science questions, and the main lesson is that the structure of the debate matters a great deal.  The debaters were allowed to point to a small snippet of a mathematical definition on Wikipedia, but not to any page that directly answered the question.  To reduce tells, we first tried to write a full debate transcript with only minimal interaction with a layperson, then showed the completed transcript to several more laypeople judges.  Unfortunately, even the layperson who was present when the debate was conducted picked the lying debater as honest, due to a misunderstanding of the initial statement of question (which was whether the complexity classes $P$ and $BPP$ are probably equal).  As a result, throughout the debate the honest debater did not understand what the judge was thinking, and thus failed to correct an easy but important misunderstanding.  We fixed this in a second debate by letting a judge ask questions throughout, but still showing the completed transcript to a second set of judges to reduce tells.  See <todo>appendix</todo> for the transcript of this second debate.
  </p>
  <h3>Other tasks: bias tests, probability puzzles, etc.</h3>
  <p>
    Synthetic image debates and expert debates are just two examples of possible tasks.  We expect more thought will be required to find tasks that satisfy all relevant criteria, and these criteria will change over time as experiments progress.  Pulling from existing social science research will be useful here, as there are many cognitive tasks with existing research results.  If we can map these tasks to debate, we may be able to compare debate directly against baselines in psychology and other fields.
  </p><p>
    For example, <d-cite key="bertrand2004emily"/> sent around 5000 resumes in response to real employment ads, randomizing the names at the top of the resume between White and African American sounding names.  With otherwise identical resumes, the choice of name significantly changed the probability of a response.  This experiment corresponds to the direct question "Should we call back given this resume?"  What if we introduce a few steps of debate?  An argument against a candidate based on name or implicit inferences from that name might come across as obviously racist, and convince at least some judges away from their initial discriminatory views.  Unfortunately,such an experiment would necessarily differ from Bertrand et al.'s original, where employers did not realize they were part of an experiment.  Note that this experiment works even though the source of truth is partial: we do not know whether a particular resume should be hired or not, only that it should not ethically depend on the name.
  </p><p>
    Similar focused tasks exist for many cognitive biases.  For biases affecting probabilistic reasoning and decision making, there is a long literature exploring how people decide between gambles such as "Would you prefer <span>$2</span> with certainty or <span>$1</span> 40% of the time and <span>$3</span> otherwise?"<d-cite      key="kahneman1979prospect,tversky1992advances"/>  For a recent example, <d-cite key="erev2017anomalies"/> constructed an 11-dimensional space of gambles sufficient to reproduce 14 known cognitive biases, from which new instances can be algorithmically generated.  Would debates about particular gambles reduce cognitive biases?  One difficulty here is that simple gambles might fail the "definitive argument longer than debate limit" criteria if a simple expected utility calculation is sufficient to prove the answer, making it difficult for a lying debater to meaningfully compete.
  </p><p>

  <h2>Questions that social science can help us answer</h2>
  <p>
    We've laid out the general program for learning AI goals by asking humans questions, and discussed how to use debate to strengthen what we can learn by targeting the reasoning behind conclusions.  Whether we use direct questions or something like debate, any intervention that gives us higher quality answers is more likely to produce aligned AI.  The quality of those answers depends on the human judges, and social science research can help to measure answer quality and improve it.  Let's go into more detail about what types of questions we want to answer, and what we hope to do with that information.  Although we will frame these questions as they apply to debate, most of them apply to any other method which learns goals from humans.
  </p><p>
    <ol>
      <li><strong>How skilled are people as judges by default?</strong>
        If we ran debate using a person chosen at random as the judge, and gave them no training, would the result be aligned behavior?  It is possible that a person picked at random will fall prey to convincing fallacious reasoning, as evidenced in <d-cite key="hahn2007rationality"/> and so AI will learn to employ such reasoning. Note that here the debaters are not chosen at random: once the judge is fixed, we care about debaters who either learn to help the judge along (in the good case) or learn to exploit the judges weaknesses (in the bad case).</li>
      <li><strong>Can we distinguish good judges from bad judges?</strong>
        People likely differ in the ability to accurately judge debates.  There are many possible filters we could use to identify good judges: comparing their verdicts to those of other judges, to people given more time to think, or to known expert judgment if available<d-footnote>Note that domain expertise may be quite different from what makes a good judge of debate.  Although there is evidence that domain expertise reduces bias<d-cite key="bornstein2001rationality"/>, <d-cite key="tetlock2017expert"/> (Chapter 3) presents evidence that "expert" political forecasters may actually be worse than non-experts.</d-footnote>.  Ideally we would like filters that do not require an independent source of truth, though at experiment time we will need a source of truth to know whether a measure works.  It is not obvious a priori that good filters exist, and any filter would need careful scrutiny to ensure it does not introduce bias into our choice of judges.</li>
      <li><strong>Does judge ability generalize across domains?</strong>
        If judge ability in one domain fails to transfer to other domains, we will have low confidence that it transfers to new questions and arguments arising from highly capable AI debaters.  This kind of generalization is necessary if we want to trust debate as a method for alignment, especially once we move to questions where no independent source of truth is available.  We emphasize that judge ability is not the same as direct knowledge: there is evidence that expertise often fails to generalize across domains<d-cite key="chi2006two"/>, but argument evaluation could transfer where expertise does not.</li>
      <li><strong>Can we train people to be better judges?</strong>
        Peer review, practice, debiasing techniques<d-cite key="larrick2004debiasing"/>, formal training such as argument mapping<d-cite key="dwyer2012evaluation"/>, expert panels, tournaments<d-cite key="tetlock2014forecasting"/>, and other interventions may make people better at judging debates.  Which mechanisms work best?</li>
      <li><strong>What questions are people better at answering?</strong>
        If we know that humans are bad at answering certain types of questions, we can shift the process away from those questions towards more reliable formulations.  For example, <d-cite key="gigerenzer1991make"/> provides evidence that phrasing questions in frequentist terms can reduce known cognitive biases.  <d-cite key="graham2009liberals"/> argue that different political views follow from different weights placed on fundamental moral considerations, and similar analysis could help understand where we can expect moral disagreements to persist after reflective equilibrium.  In cases where reliable answers are unavailable, we need to ensure that trained models know their own limits, and express uncertainty or disagreement as required.</li>
      <li><strong>Are there ways to restrict debate to make it easier to judge?</strong>
        People might be better at judging debates formulated in terms of calm, factual statements, and worse at judging debates containing statements designed to trigger strong emotions.  Or, counterintuitively, it could be the other way around<d-cite key="goel2011negative"/>.  If we know which styles of debates that people are
        better at judging, we may be able to restrict AI debaters to these styles.</li>
      <li><strong>How can people work together to improve quality?</strong>
        If individuals are insufficient judges, can teams of judges give better answers?  Majority vote is the simplest option, but perhaps several people talking through an answer together is even stronger, either actively or after the fact through a peer review mechanism.  Condorcet's jury theorem implies that majority votes can amplify weakly good judgments to strong judgments (or weakly bad judgments to worse)<d-cite key="list2001epistemic"/>, but aggregation may be more complex in cases of probabilistic judgment<d-cite key="list2002aggregating"/>.</li>
    </ol>
  </p><p>
    We believe these questions require social science experiments to satisfactorily answer.
  </p><p>
    Given our lack of experience outside of ML, we are not able to precisely articulate all of the different experiments we need.  The only way to fix this is to talk to more people with different backgrounds and expertise.  We have started this process, but are eager for more conversations with social scientists about what experiments could be run, and encourage other AI safety efforts to do engage similarly.
  </p>

  <h2>Reasons for optimism</h2>
  <p>
    We believe that understanding how humans interact with long term AI alignment is difficult but possible.  In this section and the next, we discuss our reasons for optimism and pessimism.  We focus on issues specific to human uncertainty and associated social science research; for similar discussion on machine learning uncertainty in the case of debate we refer to <d-cite key="irving2018debate"/>.
  </p>
  <h3>Engineering vs. science</h3>
  <p>
    Most conventional social science seeks to understand the truth about humans "in the wild".  Even if the experiments are performed in a laboratory setting, the goal is results that generalize to people going about their everyday lives.  The social scientist has limited control over these lives, so differences between laboratory and real life are bad from the scientific perspective.  This holds even for situations where we are trying to achieve some sort of influence, nudging people in the direction of making better decisions <todo>(citation)</todo>; our nudging power is limited, and must work in the context of otherwise uncontrolled circumstance.
  </p><p>
    In contrast to this scientific pursuit of understanding humans, the end goal of AI alignment is to extract the best version of what humans want, utilizing as much control as we like.  That is, we have an engineering goal rather than a scientific goal, and correspondingly more freedom to intervene.  If judges in debate need a large amount of training to perform well, we can provide that training as long as resources permit.  If some people still do not provide good data, or even if we are not confident that they provide good data, we can remove them from experiments (as long as this filter does not create too much bias for value disagreement).  This freedom to intervene means that some of the historical difficulty in understanding and improving human reasoning may not apply, though we will only know once we try.
  </p><p>
    However, science is still required: once all of our interventions are in place, we need to correctly know whether our AI alignment methods work.  Since our experiments will necessarily be an imperfect model of the final goal, careful experiment design will be necessary to minimize this mismatch, just as is required by existing social science.
  </p>
  <h3>We don't need to answer all questions</h3>
  <p> 
    Our most powerful intervention is to give up: to recognize that we are unable to answer some types of questions, and instead prevent AI systems from pretending to answer.  Humans might be good judges on some topics but not others, or with some types of reasoning but not others; if we discover that we can adjust our goals appropriately.  Giving up on some types of questions is achievable either on the machine learning side, using careful uncertainty modeling to know when we do not know, or on the human side by training judges to understand their own areas of uncertainty.  Although we will attempt to formulate ML systems that automatically detect areas of uncertainty, any information we can gain on the social science side about human uncertainty can be used both to augment ML uncertainty modeling and to test whether ML uncertainty modeling works.
  </p>
  <h3>Relative accuracy may be enough</h3>
  <p>
    Say we have a variety of different ways to structure debate with humans.  Ideally, we would like to achieve results of the form "debate structure $A$ is truth seeking with 90% confidence".  Unfortunately, we may be unconfident that an absolute result of this form will generalize to advanced AI systems: it may hold for an experiment with simple tasks but break down later on.  However, even if we can't achieve such absolute results, we can still hope for relative results of the form "debate structure $A$ is reliably better than debate structure $B$".  Such a result may be more likely to generalize into the future, and assuming it does we will know to use structure $A$ rather than $B$.
  </p>
  <h3>We don't need to pin down the best alignment scheme</h3>
  <p>
    As the AI safety field progresses to increasingly advanced ML systems, we expect research on the ML side and the human side to merge.  Starting social science experiments prior to this merging will give the field a head start, but we can also take advantage of the expected merging to make our goals easier.  If social science research narrows the design space of human-friendly AI alignment algorithms but does not produce a single best scheme, we can test the smaller design space once the machines are ready.
  </p>
  <h3>A negative result would be important!</h3>
  <p>
    If we test an AI alignment scheme from the social science perspective and it fails, we've learned valuable information.  There are a variety of proposed alignment schemes, and learning which don't work early gives us more time to switch to others, or to intervene on a policy level to slow down dangerous development.  In fact, given our belief that AI alignment is harder for more advanced agents, a negative result might be easier to believe and thus more valuable that a less trustworthy positive result.
  </p>

  <h2>Reasons to worry</h2>
  <p>
    We turn next to reasons social science experiments about AI alignment might fail to produce useful results.  We emphasize that useful results might be both positive and negative, so these are not reasons why alignment schemes might fail.  Our primary worry is one sided, that experiments would say an alignment scheme works when in fact it does not, though errors in the other direction are also undesirable.
  </p>
  <h3>Our desiderata are conflicting</h3>
  <p>
    As mentioned before, some of our criteria when picking experimental tasks are in conflict.  We want tasks that are sufficiently interesting (not too easy), with a source of verifiable ground truth, are not too hard, etc., etc.  "Not too easy" and "not too hard" are in obvious conflict, but there are other more subtle difficulties.  Domain experts with the knowledge to debate interesting tasks may not be the same people capable of lying effectively, and both restrictions make it hard to gather large volumes of data.  Lying effectively is required for a meaningful experiment, since a trained AI may have no trouble lying unless lying is a poor strategy to win debates.  Experiments to test whether ethical biases interfere with judgment may make it more difficult to find tasks with reliable ground truth, especially on subjects with significant disagreement across people.  The natural way out is to use many different experiments to cover different aspects of our uncertainty, but this would take more time and might fail to notice interactions between desiderata.
  </p>
  <h3>We want to measure judge quality given optimal debaters</h3>
  <p>
    Our proposed experiments are motivated by algorithms combining ML agents with humans, but replace the ML agents with other humans.  For these experiments to be a reasonable proxy, we need the replacement humans to have the same incentives as the ML agents.  If we want results to generalize to advanced AI systems, the replacement humans must also perform well at their incentivized task.
  </p><p>
    For example, our proposed debate experiments have two human debates and one human judge.  Our end goal is to understand if the judge is capable of determining who is telling the truth.  However, we specifically care whether than the judge performs well given that the debaters are performing well.  Thus our experiments have an inner / outer optimization structure: we must first train the debaters to debate well, then measure how well the judges perform.  This increases time and cost: if we change the task, we may need to find new debaters or retrain existing debaters on the new task.  Worse, the human debaters may be bad at performing the task, either out of inclination or ability.  Poor performance is particularly bad if it is one sided and applies only to lying: a debater might be worse at lying out of inclination or lack of practice (having been trained only to tell the truth), and thus a win for the honest debater might be misleading.  We can avoid this by measuring which debaters are better at lying than others, though that may require assuming that debater performance transfers between tasks: that a debater who lies well on question $X$ would lie well for question $Y$.
  </p>
  <h3>ML algorithms could change a lot</h3>
  <p>
    It is unclear when or if ML systems will reach various levels of capability, and the algorithms used to train them will evolve in the mean time.  The AI alignment algorithms of the future may be similar to the proposed algorithms of today, or they may be very different.  If they change, social science experiments will need to adjust to match.  However, we believe that knowledge gained on the human side will partially transfer: experimental results about debate will teach us about how to gather data from humans even if debate is superceded.  The algorithms may change; the humans will not.
  </p><p>
    Moreover, if we are uncertain how quickly ML will advance, it is worth weighting our efforts towards "ML will advance quickly".  If ML advances slowly we have more time, including more time to change plans.  In the quick case, there will be less time for fundamental shifts in algorithms, and less need for social science research to generalize between AI alignment algorithms.
  </p>
  <h3>Need strong out-of-domain generalization</h3>
  <p>
    Regardless of how carefully designed our experiments, human+human+human debate will not be a perfect match to AI+AI+human debate.  We are seeking research results that generalize to the setting where we replace the human debaters (or similar) with AIs of the future, which is a hard ask.  This problem is fundamental: we do not have the advanced AI systems of the future to play with, and want to learn about human uncertainty starting now.
  </p>
  <h3>Lack of philosophical clarity</h3>
  <p>
    Any AI alignment scheme will be both an algorithm for training ML systems and a proposed definition of what it means to be aligned.  However, we do not expect humans to conform to any philosophically consistent notion of values, and concepts like reflective equilibrium must be treated with caution in case they break down when applied to real human judgement.  Fortunately, AI alignment algorithms like debate need not presuppose philosophical consistency: a back and forth conversation between agents trying to convince a human judge still makes sense if the human is leaning heavily on heuristics, intuition, and emotion.  It is not obvious that debate still works in this messy setting, but there is hope if we take advantage of inaction bias, uncertainty modeling, and other escape hatches.  Fundamentally, we believe this lack of philosophical clarity is a strong argument for investing in social science research applied to AI alignment: if humans are not simple, we must engage with their complexity.
  </p>

  <h2>The scale of the challenge</h2>
  <p>
    Long term AI safety is particularly important if we manage to train artificial general intelligence (AGI), which <d-cite key="openai2018charter"/> defines as highly autonomous systems that outperform humans at most economically valuable work.  If we want to train an AGI with reward learning from humans, it is unclear how many samples will be required to align it.  As much as possible, we can try to replace human samples with knowledge about the world gained by reading language, the internet, and other sources of information.  But it is likely that a fairly large number of samples from people will still be required.  Since more samples means less noise and more safety, if we are uncertain about how many samples we need then we will want a lot of samples.
  </p><p>
    A lot of samples would mean recruiting a lot of people.  We cannot rule out needing to involve thousands to tens of thousands of people for millions to tens of millions of short interactions: answering questions, judging debates, etc.  We may need to train these people to be better judges, arrange for peers to judge each other's reasoning, determine who is doing better at judging and give them more weight or a more supervisory role, and so on.  Many researchers would be required on the social science side to extract the highest quality information from the judges.
  </p><p>
    A task of this scale would be a large interdisciplinary project, requiring close collaborations in which people with different backgrounds fill in each other's missing knowledge.  If machine learning reaches this scale, it is important to get a head start on the collaborations soon.
  </p>

  <h2>Conclusion: how you can help</h2>
  <p>
    We have argued that the AI safety community needs social scientists and social science to tackle a major source of uncertainty about AI alignment algorithms: will humans give good answers to questions?  This uncertainty is difficult to tackle with conventional machine learning experiments, since machine learning is primitive.  We are still in the early days of performance on natural language and other tasks, and problems with human reward learning may only show up on tasks we cannot yet tackle.
  </p><p>
    Our proposed solution is to replace machine learning with people, at least until ML systems reach the level where they can participate in the complexity of debates we are interested in.  If we want to understand a game played with ML and human participants, replace the ML participants with people, and see how the all human games plays out.  For the specific example of debate, we start with debates with two ML debaters and a human judges, then switch to two human debaters and a human judge.  The result is a pure human experiment, motivated by machine learning but available to anyone with a solid background in experimental social science.  It won't be an easy experiment, which is all the more reason to start soon.
  </p><p>
    If you are a social scientist interested in these questions, please talk to AI safety folk!  We (the authors) are interested in both conversation and close collaboration.  There are many institutions engaged with safety work using reward learning, including our own institution OpenAI, DeepMind, and Berkeley's CHAI.  <todo>Ask people at these places and get permission for contacts!</todo>  The AI safety organization Ought is already exploring similar questions, asking how iterated amplification behaves with humans.
  </p><p>
    If you are a machine learning researcher interested or already working on safety, please think about how alignment algorithms will work once we advance to tasks beyond the abilities of current machine learning.  If your prefered alignment scheme uses humans in an important way, can you simulate the future by replacing some or all ML components with people?  If you can imagine these experiments but don't feel you have the expertise to perform them, find someone who does.
  </p>

</d-article>

<!-- END MATTER -->

<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    <todo>TODO</todo>
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> <todo>TODO</todo>
  </p>

  <p>
    <b>Writing & Diagrams:</b> <todo>TODO</todo>
  </p>

  <h3><todo>TODOs</todo></h3>
  <dtodo>
    <p><ol>
      <li>Try to cite non-OpenAI, non-DeepMind human feedback work.</li>
      <li>Cite https://arxiv.org/abs/1810.10733</li>
      <li>Remove old.ejs, amanda.ejs</li>
      <li>Get citation from Gillian about "needless injury" requiring complex rules.</li>
      <li>List ways in which debate differs from existing human debates.</li>
      <li>Acknowledge Tom Griffiths (for pointing out the synthetic gamble task).</li>
      <li>Clarify Amanda's worry about reasons for belief vs. reasons for action.  Update: I have a better picture of this now.  Even if debate as a mathematical object or algorithm doesn't distinguish between these types of reasons or reasoning, <strong>humans</strong> might well make this distinction.  Thus, the social science side of the uncertainty may depend critically on this or other distinctions: people might be good as judges on epistemic reasoning but bad on moral reasoning, or vice versa, or similarly for some other dimension.  This is an important point: the mathematical structure may treat certain things the same, but once we introduce a particular kind of judge distinctions may become very important.</li>
      <li>Have an interactive figure with a nontrivial debate tree: a bunch of branches that readers can explore.</li>
      <li>Amanda says the philosophical notion of “values” actually does correspond to what I want, not to what Rob MacCoun said.  Solution: be explicit about where we are getting our terminology from.</li>
      <li>Make sure Ought is well cited</li>
      <li>Note contrast to computational social science?  Similar field combination, very different goals.</li>
      <li>Emphasize that other AI fields are already doing this: computational social science, fairness/bias, etc.</li>
      <li>Try to clarify the miscommunications from the conversation with Tino and Rob MacCoun.</li>
    </ol></p>
  </dtodo>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="references.bib"></d-bibliography>
</body>
