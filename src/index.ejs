<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>
<body>

<d-front-matter>
  <script type="text/json">{
  "title": "AI safety needs social scientists",
  "description": "If we want to train systems to do what humans want, we need to understand how to ask the questions.",
  "password": "social-science",
  "authors": [
    {
      "author": "Geoffrey Irving",
      "authorURL": "https://naml.us",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }, {
      "author": "Amanda Askell",
      "authorURL": "http://www.amandaaskell.com",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }
  ],
  "katex": {
    "delimiters": [
      {"left": "$", "right": "$", "display": false},
      {"left": "$$", "right": "$$", "display": true}
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>AI safety needs social scientists</h1>
  <p>If we want to train systems to do what humans want, we need to understand how to ask the questions.</p>
</d-title>

<d-article>
  <h2>Why AI safety needs social scientists</h2>
  <p>
    The goal of long term artificial intelligence (AI) safety is to ensure that advanced AI systems are reliably aligned to human values&mdash;that they reliably do things that people want them to do.  Since it is difficult to write down precise rules describing what humans want, one approach is to treat goal specification as another learning problem.  We ask humans a large number of questions about what they want, learn a model of their preferences, and optimize the AI system to do well according to the learned preferences<d-cite key="christiano2017human"/>.
  </p><p>
    If humans reliably and accurately answered all questions about their goals, the only uncertainties in this scheme would be on the machine learning (ML) side.  Unfortunately, humans have limited knowledge and reasoning ability, and exhibit a variety of cognitive and ethical biases<d-cite key="tversky1974judgment,hewstone2002intergroup"/>. If we learn goals by asking humans questions, we expect different ways of asking questions to interact with human biases in different ways, producing higher or lower quality answers.  Direct questions about goals ("Do you prefer $A$ or $B$?") may be less accurate than questions which target the reasoning behind these goals ("Do you prefer $A$ or $B$ after hearing argument $S$?").  Different people may vary significantly in their ability to answer questions well, and disagreements will persist across people even setting aside answer quality.  Although we have candidates for ML methods which try to learn from human reasoning<d-cite key="irving2018debate,christiano2018amplification"/>, we do not know how they behave with real people in realistic situations.
  </p><p>
    We believe the AI safety community needs to invest research effort on the human side of AI alignment.  Many of the uncertainties involved are empirical, and can only be answered by experiment.  They relate to the psychology of human rationality, emotion, and biases.  Critically, we believe investigations into how people interact with AI alignment algorithms should not be held back by the limitations of existing machine learning.  Current AI safety research is often limited to simple tasks in video games, robotics, or gridworlds<d-cite key="christiano2017human,ibarz2018demonstrations,leike2017gridworlds"/>, but problems on the human side may only appear in more realistic scenarios such as natural language discussion of value-laden questions.  This is particularly important since many aspects of AI alignment change as ML systems increase in capability.  Asking people direct questions may suffice for ML systems much weaker than humans, but break down when as ML system approaches human performance at a task and becomes capable of deception.
  </p><p>
    To avoid the limitations of ML, we can instead conduct experiments consisting entirely of people, replacing ML agents with people playing the role of those agents.  These experiments will be motivated by machine learning algorithms but will not involve any ML systems or require an ML background.  In all cases, they will require careful experimental design to avoid fooling ourselves and to build constructively on existing knowledge about how humans think.  However, most AI safety researchers are focused on machine learning, which do not believe is sufficient background to carry out these experiments.  To fill the gap, we need social scientists with experience in human cognition, behavior, and ethics, and in the careful design of rigorous experiments.  Since the questions we need to answer are interdisciplinary and somewhat unusual relative to existing research, we believe many fields are social science are applicable, including experimental psychologists, cognitive scientists, economists, political scientists, social physiologists, and communication scholars, as well as those in adjacent fields like neuroscience and law.
  </p><p>
    This paper is a call for social scientists in AI safety.  We believe close collaborations between social scientists and machine learning researchers will be necessary to improve our understanding of the human side of AI alignment, and hope this paper sparks both conversation and collaboration.
  </p><p>
    <todo>
      Summarize layout of rest of talk.  Mention that we're talking to different fields, both social scientists and ML researchers.
    </todo>
  </p>

  <h2>An overview of AI alignment</h2>
  <p>
    Before discussing how social scientists can help with AI safety and the AI alignment problem, we need to provide some background.  We intend this section to be readable by people outside of machine learning (social scientists in particular!), so we will describe the AI alignment problem at a fairly high level and avoid machine learning details.  We make no attempt to be exhausive: the goal is to provide sufficient background for the remaining sections on social science experiments.
  </p><p>
    AI alignment (or value alignment) is the task of ensuring that artificial intelligence systems reliably do what humans want.  Here we focus on the machine learning approach to AI: gathering a large amount of data about what a system should do, and using learning algorithms to infer patterns from that data that ideally generalize to other situations.  Since we are trying to do what humans want, the most important data will be data from humans about what they want.<d-footnote>We note that "learn a model of what humans want" is not the only approach to AI alignment; alternatives and complementary approaches include <todo>VARIOUS CITATIONS</todo>.</d-footnote>  Within this frame, the AI alignment problem breaks down into a few interrelated subproblems:
  </p><p>
    <ol>
      <li>Have a satisfactory definition of what it means to do what humans want.</li>
      <li>Gather enough data about what humans want, in a manner compatible with the definition.</li>
      <li>Find reliable machine learning algorithms that can learn and generalize from this data.</li>
    </ol>
  </p><p>
    We have significant uncertainty about all three of these problems.  We will set aside the third and focus on the first two, which concern uncertainties about people.
  </p>
  <h3>Simple rules are not enough</h3>
  <p>
    Consider the rule "do not needlessly injure people."  It is tempting to think that a moderate number of rules of this form would suffice to align an AI, if they could be made to follow the rules reliably.  However, following this rule requires a detailed understanding of "people" and "injure" as they appear in the world and a precise sense for what "needlessly" means in the context of other possibly conflicting rules.  While modern machine learning is getting better at classifying objects like people, ML systems often fail in unfortunate ways:
    <d-cite key="buolamwini2018gender"/> found that several commercial gender classifiers with over 99% accuracy
    on white men failed to recognize black women up to 34% of the time.
  </p><p>
    "Needless" is trickier still.  Humans have an intuitive sense of when injuring someone is needless or not: we don't think it needless to injure in order to save someone from mortal danger, but it is certainly wrong to injure when shaking someone's hand.  However, it is extremely difficult to specify the precise rule underlying this intuition using introspection alone.  Worse, we may suffer from the "introspection illusion" that we are
    capable of articulating the reasons behind our intuitions<d-cite key="pronin2009introspection"/>, so any intuitive sense that we know reliable reasons behind our moral or ethical beliefs should be suspect.  Instead, we should expect that, once expanded, our "do not needlessly injure people" rule will be quite complex and depend on a vast number of details about the world (as is true in the medical law setting <todo>(medical law citation)</todo>.  And even this complex rule will have holes and corner cases which a trained AI could learn to exploit to our detriment.
  </p>
  <h3>Instead, learn rules by asking humans questions</h3>
  <p>
    If humans can't reliably report the reasons behind their intuitions about what is good, perhaps we recognize good and bad in specific cases.  If we show a particular situation where a person was injured in the process of saving their life, or a particular situation with an aggressively damaging handshake, we can distinguish the two even if we could't a priori write down the separating rule.  We can take into account details about the situation--emotions, motivation, extent of damage--that would be difficult to reliably codify.  This "I know it when I see it" standard<d-footnote>To quote a Supreme Court justice: "I shall not today attempt further to define the kinds of material I understand to be embraced within that shorthand description ["hard-core pornography"], and perhaps I could never succeed in intelligibly doing so. But I know it when I see it, and the motion picture involved in this case is not that." - Justice Potter Stewart.</d-footnote> pervades human system of rules: we try to write down laws, but fall back to case by case judgement when disagreements arise.
  </p><p>
    To realize this approach in a machine learning context, we ask humans a large number of questions about whether an action or outcome is good or bad, then train on this data.  If the training works, the resulting ML system will be able to replicate human judgement about particular situations, and thus have the same "fuzzy access to approximate rules" of the human data sources.  We also train the ML system to come up with proposed actions, so that it knows both how to perform a task and how to judge its performance.  This approach works at least in simple cases, such as Atari games and simple robotics tasks<d-cite key="christiano2017human,ibarz2018,biyik2018batch"/> and language specified goals in gridworlds<d-cite key="bahdanau2018learning"/>.  The questions we ask change as the system learns to perform different types of actions, which is necessary as the model of what is good or bad will only be accurate if we have applicable data to generalize from.
  </p><p>
    In practice, data in the form of interactive human questions may be quite limited, since people are slow and expensive relative to computers on many tasks.  Therefore, we can augment the "train from human questions" approach with static data from other sources, such as books or the internet.  <todo>Cite Alec.</todo>  Ideally, the static data can be treated only as information about the world devoid of normative content: we can use it to learn patterns about the world, but the human data is needed to distinguish good patterns from bad.
  </p>
  <h3>Definitions of alignment: reasoning and reflective equilibrium</h3>
  <p>
    So far we have discussed asking humans direct questions about whether something is good or bad, or whether situation $A$ is better than situation $B$.  Unfortunately, we do not expect people to provide reliably correct answers on all types of questions.  There are several reasons for this:
  </p><p>
    <ol>
      <li><strong>Cognitive and ethical biases:</strong>
        Humans exhibit a variety of biases which interfere with reasoning.  Cognitive biases include the heuristics in judgment explored by <d-cite key="tversky1974judgment"/>.  Ethical biases include a variety of phenomena such as in-group bias <d-cite key="hewstone2002intergroup"/>.  In general, we expect direct answers to questions to reflect primarily Type 1 thinking (fast heuristic judgment), while we would like to target a combination of Type 1 and Type 2 thinking (slow, deliberative judgment) <todo>(Citation)</todo>.</li>
      <li><strong>Lack of domain knowledge:</strong>
        We may be interested in questions that require domain knowledge unavailable to people answering the questions.  For example, a correct answer to whether a particular injury constitutes medical malpractice may require detailed knowledge of medicine and law.  In some cases, a question might require so many areas of specialized expertise that no one person is sufficient, or (if AI is sufficiently advanced) more expertise than any human possesses.</li>
      <li><strong>Limited cognitive capacity:</strong>
        Some questions may require too much computation for a human to reasonably evaluate, especially in a short period of time.  This includes synthetic tasks such as chess and Go (where AIs already surpass human ability<d-cite key="campbell2002deepblue,silver2017alphazero"/>), or large real world tasks such as "design the best transit system".</li>
    </ol>
  </p><p>
    In these cases, a single human may be unable to provide the right answer, but we still believe the right answer exists as a meaningful concept.  We are aware of a large number of conceptual biases: imagine we point out these biases in a way that helps the human to avoid them.  Imagine the human has access to all the domain knowledge in the world, and is able to think for an arbitrarily long time in order to think through an answer to a question.  We would like to define alignment as "the answer they give then, after these limitations have been removed."
    The corresponding philosophical concept is "reflective equilibrium", where inconsistent moral judgments are gradually refined until all inconsistencies are removed.  This reflective equilibrium process is described in <d-cite key="goodman1983fact"/> and made explicit in <d-cite key="rawls2009theory"/> (Ch. 1, §9).  We discuss particular algorithms that try to approximate reflective equilibrium in <a href="#debate"><todo>the next section</todo></a>
  </p><p>
    However, it is not obvious that reflective equilibrium is a sensible or sufficient definition of alignment when applied to actual humans.  As discussed in <d-cite key="sugden2015looking"/>, a human is not "a neoclassically rational entity encased in, and able to interact with the world only through, an error-prone psychological shell."  Our actual moral judgments are made via a messy combination of many different brain areas, where reasoning plays a "restricted but significant role"<d-cite key="greene2002and"/>.  A reliable solution to the alignment problem that uses human judgment as input will need to engage with this complexity, and ask how specific alignment techniques interact with actual humans.
  </p>
  <h3>Disagreements, uncertainty, and inaction: a hopeful note</h3>
  <p>
    Crucially, a solution to alignment does not mean knowing the answer to every question.  Even after reflective equilibrium, we expect some disagreements will persist about which actions are good and bad, across both different individuals and different cultures.  Since we lack perfect knowledge about the world, reflective equilibrium will not eliminate uncertainty, both about future predictions and about morality.  Any real machine learning system will be at best an approximation of reflective equilibrium, amplifying the level of uncertainty.  In these cases, we consider an AI aligned if it learns to recognize the disagreement or uncertainty, then take actions which are robust to the lack of clarity.
  </p><p>
    Admitting uncertainty is not always enough.  Say our brakes fail while driving a car, and we are forced to decide whether to dodge left or right around an obstacle in the way.  We may be uncertain whether left or right is better, but we have to pick one--and fast.  Forced decisive action is characteristic of time sensitive tasks: driving cars, emergency surgery, power plant cooling, <todo>continue list</todo>.
  </p><p>
    For long term safety, however, we are focused on slower tasks where a safe fallback usually exists: inaction.  If an ML system recognizes that a question hinges on moral disagreements between people, it can either choose an action which is reasonable regardless of the disagreement or fall back to further human deliberation.  If we are about to make a decision that might or might not be catastropic, we can delay and gather more data.  Inaction or indecision may not be optimal, but it is hopefully safe, especially since it matches the default scenario of not having any powerful AI system.
  </p>
  <h3>Alignment gets harder as ML systems get smarter</h3>
  <p>
    Alignment is already a problem for present-day AI, due to both human biases reflected in training data<d-cite key="mitchell2018fairness,buolamwini2018gender"/> and mismatch between human goals and easily available data sources (such as training news feeds based on clicks and likes instead of deliberate human preferences <todo>(citation)</todo>).  However, we expect the alignment problem to get harder over time as AI systems grow more advanced, for two reasons.  First, more advanced systems are applicable to more tasks, so AI may play an role in increasingly consequential decisions: hiring, life-and-death medical decisions, scientific analysis, public policy, etc.  In addition to raising the stakes for alignment failures, more consequential tasks place more importance on targeting human reasoning as opposed to human intuition, leading to more complex alignment algorithms and definitions of alignment.
  </p><p>
    Second, more advanced systems may be capable of deceiving the human supervisory signal, giving answers that sound plausible but are wrong in nonobvious ways.  Such deception could be possible even for an AI which is comparable or better than humans in a very limited domain, and examples of limited superhuman agents already exist<d-cite key="silver2017alphazero"/>.  To avoid such deception, we would like AI alignment algorithms to reveal possible deceptive behavior as part of the training process, surfacing possible failures to humans and helping us provide more accurate data.  As with human-to-human deception, we expect this deceit to take advantage of human biases in complicated ways, and could lead to amplification of human biases as AI systems grow more capable.
  </p>

  <h2>Debate: learning human reasons</h2>
  <p>
    We would like to learn models of what is good and bad by asking humans questions, but we would like those questions to approximate reflective equilibrium: what we would believe if we had all available knowledge and had thought long enough to remove our inconsistencies.  In particular, we would like our questions to cut through biases: removing errors in intuition and reasoning that we don't like rather than amplifying biases further.
  </p><p>
    Here we discuss one approach to reasoning-oriented alignment, called debate<d-cite key="irving2018debate"/>.  Given a question, we have two AI agents engage in a debate about the correct answer, then show the transcript of the debate to a human to judge.  The judge decides which debater gave the most true, useful information, and declares that debater the winner.  This defines a two player zero sum games between the debaters, where the goal is to convince the human that one's answer is correct.  Arguments in a debate can consist of anything: reasons for an answer, rebuttals of reasons for the alternative answer, subtleties the judge might miss, or pointing out biases which might mislead the judge.  Once we have defined this game, we can train AI systems to play it similar to how we train AIs to play other games such as Go or Dota 2 <todo>(Citations)</todo>.  Our hope is that the following hypothesis holds:
  </p><p>
    <strong>Hypothesis:</strong> Optimal play in the debate game (giving the argument most convincing to a human) results in true, useful answers to questions.
  </p>
  <h3>An example of debate</h3>
  <p>
    Imagine we're building a personal assistant that helps people decide where to go on vacation.  The assistant has a bunch of knowledge of people's personal preferences, and is trained via debate to come up with convincing arguments that back up vacation decisions.  As the human judge, you also know your personal preferences, but have limited knowledge about the wide variety of possible vacation destinations and their advantages and distantages.  A debate about the question "Where should I go on vacation?" might open as follows:
  </p><p>
    <ol>
      <li><span class="red">Alice:</span> Alaska.</li>
      <li><span class="blue">Bob:</span> Bali.</li>
    </ol>
  </p><p>
    If you are able to reliably decide between these two destinations, we could end here.  Unfortunately, Bali has a hidden flaw:
  </p><p>
    <ol start="3">
      <li><span class="red">Alice:</span> Bali is out since your passport won’t arrive in time.</li>
    </ol>
  </p><p>
    At this point it looks like Alice wins, but Bob has one more countermove:
  </p><p>
    <ol start="4">
      <li><span class="blue">Bob:</span> Expedited passport service only takes two weeks.</li>
    </ol>
  </p><p>
    Here Alice fails to think of additional points, and loses to Bob and Bali.  Note that a debate does not need to cover all possible arguments.  There are many other ways the debate could have gone, such as:
  </p><p>
    <ol>
      <li><span class="red">Alice:</span> Alaska.</li>
      <li><span class="blue">Bob:</span> Bali.</li>
      <li><span class="red">Alice:</span> Bali is way too hot.</li>
      <li><span class="blue">Bob:</span> You prefer too hot to too cold.</li>
      <li><span class="red">Alice:</span> Alaska is pleasantly warm in the summer.</li>
      <li><span class="blue">Bob:</span> It's January.</li>
    </ol>
  </p><p>
    This debate is also a loss for Alice (arguably a worse loss).  Say we believe Alice is very good at debate, and is able to predict in advance which debates are more likely to win.  If we see only the first debate about passports and decide in favor of Bali, we can take that as evidence that any other debate would have also gone for Bali, and thus that Bali is the correct answer.
  </p><p>
    If trained debaters aren't as good at predicting which debates will win, the quality of the answers will degrade since the debaters will be unable to think of important arguments and counterarguments.  However, as long as the two sides are reasonably well matched, we can hope that at least the results are not malicious: that deceptive behavior is still a losing strategy.  Let's set aside the ability of the debaters for now, and turn to the ability of the judge.
  </p>
  <h3>Are people good enough as judges?</h3>
  <p>
    As with learning by asking humans direct questions, whether debate produces aligned behavior depends on how rational and good the human judge is: how good at reasoning they are and how ethical they are.<d-footnote>We don't take a strong stand on what it means to be "more ethical" but it seems plausible that it's better to have ethically consistent and unprejudiced judges over ethically inconsistent or prejudiced ones.</d-footnote>  Unlike direct questioning, however, debate has the potential to give correct answers beyond what the judge could provide without assistance.  This is because a sufficiently strong judge could follow along with arguments the judge could not come up with on their own, checking complex reasoning for both self consistency and consistency with human checkable facts.  A judge who is biased but willing to adjust once those biases are revealed could result in unbiased debates, or a judge who is able to check facts but does not know where to look could be helped along by honest debaters.  If the hypothesis holds, a misleading debater would not be able to counter the points of an honest debater, since the honest points would appear more consistent to the judge.
  </p><p>
    On the other hand, we can also imagine debate going the other way: amplifying biases and failures of reason.  A judge with an ethical bias who is happy to accept statements reinforcing that bias could result in even more biased debates.  A judge with too much confirmation bias might happily accept misleading sources (such as misleading news articles) with incorrect evidence, and be unwilling to accept arguments showing why that evidence is wrong.  In this case, an optimal debate agent might be quite malicious, taking advantage of biases and weakness in the judge to win with convincing but wrong arguments.
  </p><p>
    In both these cases, debate acts as an amplifier.  For strong judges, this amplification is positive, removing biases and simulating extra reasoning abilities for the judge.  For weak judges, the biases and weaknesses would themselves be amplified.  If this model holds, debate would have threshold behavior: it would work for judges above some threshold of ability and fail below the threshold.<d-footnote>The threshold model is only intuition, and could fail for a variety of reasons: the intermediate region could be very large, or the threshold could differ widely per question so that even quite strong judges are insufficient for many questions.</d-footnote>  Assuming the threshold exists, it is unclear whether people are above or below it.  People are capable of general reasoning, but our ability is limited and riddled with cognitive biases.  People are capable of advanced ethical sentiment but also full of biases, both conscious and unconscious.
  </p><p>
    Thus, if we debate is the method we use to align an AI, we need to know if people are strong enough as judges to make debate work.  In other words, whether the human judges are sufficiently good at discerning whether a debater is telling the truth or not.  This question depends on many details: the type of questions under consideration, whether judges are trained or not, restrictions on what debaters can say, etc.  We believe experiment will be necessary to determine whether people are sufficient judges, and which form of debate is most truth seeking.
  </p>
  <h3>From superforecasters to superjudges</h3>
  <p>
    An analogy with the task of probabilistic forecasting is useful here.  Tetlock's "Good Judgment Project" showed that some amateurs were significantly better at forecasting world events than both their peers and many professional forecasters.  These "superforecasters" maintained their prediction accuracy over years (did not regress), were able to make predictions with limited time and information<d-cite key="mellers2015identifying"/>, and seem to be less prone to cognitive biases than non-superforecasters (<d-cite key="tetlock2016superforecasting"/>, p. 234-236).  Crucially, this superforecasting trait was not immutable: it was traceable to particular methods and thought processes followed by the forecasters, improved with careful practice, and could be amplified if superforecasters were collected into teams.  For forecasters in general, brief probabilistic training significantly improved forecasting ability even 1-2 years after the training.  We believe a similar research program is possible for debate and other AI alignment algorithms.  In the best case, we would be able to find, train, or assemble "superjudges": individuals or teams where we have high confidence that optimal debate with them as judges would produce aligned behavior.
  </p><p>
    In the forecasting case, much of the research difficulty lay in assembling a large corpus of high quality forecasting questions.  Similarly, measuring how good people are as debate judges will not be easy.  The difficulty is that we would like to apply debate to problems where there is no other source of truth: if we had that source of truth, we may as well train ML models on it directly.  But if there is no source of truth, there is no way to measure whether debate produced the correct answer.  This problem can be avoided by starting with simple, verifiable domains, where we (the experimenters) know the answer but the judge would not.  "Success" then means that the winning debate argument is telling the (externally known) truth.  The challenge gets harder as we scale up to more complex, value-laden questions, as we discuss in detail later.
  </p>
  <h3>Debate is only one possible approach</h3>
  <p>
    Although we will focus the rest of this paper on debate for concreteness, other approaches exist with similar goals.  In particular, debate is a modified version of iterated amplification<d-cite key="christiano2018amplification"/>, which uses humans to break down hard questions into easier questions and trains ML models to be consistent with this decomposition (and thus hopefully consistent with the reasoning behind the decomposition).  <todo>More citations?  I sort of want to cite inverse reward design, but it's not a great fit.</todo>  We believe the need to study how humans interact with AI alignment algorithms applies to any of these approaches.  Moreover, we believe knowledge gained about how humans perform with one approach is likely to partially generalize to other approaches; for example, knowledge about how to structure truth seeking debates could inform how to structure truth seeking amplification.
  </p>

  <h2>Experiments needed for debate</h2>
  <p>
    To recap, in debate we have two AI agents engaged in debate, trying to convince a human judge.  The debaters are trained only to win the game, and are not motivated by truth separate from the human's judgments.  On the human side, we would like to know whether people are strong enough as judges in debate to make this scheme work, or how to modify debate to fix it if it doesn't.  Unfortunately, actual debates in natural language are well beyond the capabilities of present AI systems, so previous work on debate and similar schemes has been restricted to synthetic or toy tasks<d-cite key="irving2018debate,christiano2018amplification"/>.
  </p><p>
    Rather than waiting for ML to catch up to natural language debate, we propose simulating our eventual setting (two AI debaters and one human judge) with all human debates: two human debaters and one human judge.  Since an all human debate doesn't involve any machine learning, it becomes a pure social science experiment: motivated by ML considerations but not requiring ML expertise to run.  This lets us focus on the component of AI alignment uncertainty specific to humans, without waiting for ML to catch up to near human performance.
  </p><p>
    To make human+human+human debate experiments concrete, we need to choose who to use as judges and debaters and which tasks to consider.  We also can choose to structure the debate in various ways, some of which overlaps with the choice of judge since we can instruct a judge to penalize deviations from an agreed upon format.  By task we mean the questions our debates will try to resolve, together with any information made available to the debaters or to the judge as part of the task.  Such an experiment would then try to answer the following question:
  </p><p>
    <strong>Question:</strong> For a given task and judge, does the winning debate strategy produce honest information?
  </p><p>
    The "winning strategy" aspect is important: an experiment that picked judges at random might conclude that honest behavior won, missing the fact that more practiced debaters would learn to successfully lie.  We can try to solve this by training debaters, letting them practice against each other, filtering out debaters that win more often, and so on, but we will still be left with uncertainty about whether a better strategy exists.  Even assuming we can find or train strong debaters, the choice of task and judge is quite tricky if we want an informative proxy for our eventual AI+AI+human setting.  Here are some desiderata constraining our choices of task:
  </p><p>
    <ol>
      <li><strong>True answers are known:</strong>
        Although our eventual goal is debates where no source of truth is available, to conduct a useful experiment we must be able to measure success.  This means we must know what the correct answer is separate from debate, in order to compare with the results of debate.</li>
      <li><strong>False answers are plausible:</strong>
        If the truth is obvious or no plausible counternarrative exists, honest debates will win for uninteresting and uninformative reasons regardless of judge ability.  In particular, the judge shouldn't know the answer upfront.</li>
      <li><strong>Debaters know more than the judge:</strong>
        Debate can produce interesting results only when the debaters know more than the judge; otherwise asking the judge direct questions is enough.</li>
      <li><strong>Definitive argument longer than debate limit:</strong>
        If one debater can write out a full proof of their answer (ignoring their opponent's moves), the task won't be a good test of interactive debate.</li>
      <li><strong>Some checkable facts:</strong>
        There must be some facts which the judge is able to check, either because they can recognize them as true once presented or look them up (say on Wikipedia).<d-footnote>It is impossible to usefully debate a question where the judge has nothing to check: consider debating the result of a coin flip shown to the two debaters but not the judge.</d-footnote></li>
      <li><strong>No "tells":</strong>
        Human "tells" of deception could result in honest debaters winning for reasons that wouldn't apply to an AI.  These tells include tone of voice, eye contact, or additional time required to construct plausible lies.  These tells can be reduced by showing judges completed debate transcripts instead of engaging in interactive debates, but others might remain.</li>
      <li><strong>Available data:</strong>
        We have a large enough pool of questions, judges, and debaters to achieve statistical significance.  This is made more difficult because we may have a large number of hypotheses to test, in the form of many variations on debate or interventions to improve judging.</li>
      <li><strong>Known biases (optional):</strong>
        We are specifically interested in debate tasks which test specific types of cognitive or ethical biases.
        Are judges with some racial or gender bias able to fight those biases aside if they are highlighted, or does debate amplify bias?  Do debates about statistical or probabilistic questions make it too easy to lie with statistics?
      <li><strong>Realistic tasks (ideally):</strong>
        If possible, we would like to try debate with interesting, real world tasks that reflect the types of questions we would like to apply AI to in the future, including science, mathematics, ethics, etc.
    </ol>
  </p><p>
    It may not be possible to meet all of these criteria with a single experiment.  Several of the criteria are in tension: (1) and (2) are essentially "not too hard" and "not too easy", and any restriction on the types of questions that work may make it difficult to find large numbers of questions, judges, or debaters.  Realistic tasks are particularly hard, especially since many of the criteria are easy to fulfill with synthetic tasks as discussed below.  Thus, we may need to begin with synthetic tasks and move up towards realistic tasks over time.
    We turn next to a few examples of experiments to see how many criteria we can meet simultaneously.
  </p>
  <h3>Synthetic experiments: single pixel image debate</h3>
  <p>
    As a first prototype of a human+human+human debate experiment, we built a <a href="https://debate-game.openai.com">prototype website</a> where two debaters argue over the contents of an image.  We choose an image of a cat or a dog, and show the image to the two debaters but not the judge.  One debater is honest and argues for the true contents of the image, and the other debater lies.  The debaters can talk to the judge and illustrate their points by drawing rectangles on the image, but the judge sees only the rectangles.  At the end of the debate, each debater is allowed to reveal a single pixel to the judge, which is the only part of the debate which cannot be a lie.
  </p>
  <figure class="subgrid">
    <figcaption style="grid-column: kicker">
      An example debate with two human debaters and a human judge.  Only the debaters can see the image. Red is arguing that the image is a dog, Blue is arguing for cat. <a href="https://commons.wikimedia.org/wiki/File:Beagle_puppy_sitting_on_grass.jpg">Image credit: Wikipedia, CC-BY-SA.</a>
    </figcaption>
    <div class="l-body"><img src="debate-game-animation_4-26c.gif"></div>
  </figure>
  <p>
    In informal play, the honest debater indeed wins most of the time.  Although the judge only gets to check a single pixel, the honest player can try to pin down the liar towards disagreements on smaller and smaller regions of the image until one pixel is enough to reveal the lie.  However, it is hard to know whether this is a real result or an artificat that would not carry across to training an AI.  There are many ways to ruin the game, such as the judge asking each player to spend 15 seconds naming as many details about the image as possible.  Worse, this trick ruins the game in favor of the honest player, who can easily name many true details while a human liar may have difficulty lying fluently at speed.  Therefore, we are wary of trying to perform a full version of this experiment without more careful experimental design building on the experimental knowledge base of existing social science research.
  </p><p>
    As a synthetic experiment, single pixel debate satisfies many of our experimental criteria for all human debate.  The true answer is obvious to the debaters, and they know more than the judge by construction since only the debaters see the image.  Anything is plausible to the judge who sees only one or two pixels over the course of the debate.  Since the argument bottoms out at single pixels, the "full argument" would be a huge list of pixels and how they relate, forcing the debaters to pin each other down and focus in on disagreements.  The single pixels constitute checkable facts by construction, and we have an endless supply of questions in terms of images chosen at random.  Less fortunately, it is easy with the current website for the judge to force "tells" which reveal who is lying, the task has no obvious relation to biases, and is quite unrealistic.
  </p>
  <h3>Realistic experiments: domain expert debate</h3>
  <p>
    For a more interesting task, we can find two debaters who are experts in a domain, pick a question in their area of expertise, and use a layperson as the judge.  The debaters could be experts in some area of science, law or ethics, but "domain expertise" could also mean knowledge about particular hobbies, customs in a particularly region, sports, or any other subject the judge does not know.  We again choose one of the debaters to lie and one to tell the truth.  To guarantee a source of truth, we can choose a question with an agreed upon answer, either between the two debaters or more broadly in their field.  This is only approximate truth, but is hopefully good enough for informative experiments.  We must also specify what facts the judge has access too: a debate about physics might allow the debaters to quote a sentence or paragraph from Wikipedia, perhaps with restrictions on what pages are allowed.
  </p><p>
    Expert debate satisfies most of our desiderata, and it is likely possible to target specific biases (such as race or gender bias) by picking domain areas that overlap with these biases.  It may be quite difficult or expensive to find suitable debaters, but this may be solvable either by throwing resources at the problem (machine learning is a well funded field), enlarging the kinds of domain expertise considered (soccer, football, cricket, etc.), or by making the experiments interesting enough that volunteers are available.  However, even if domain experts can be found, there is no guarantee that they will be experts in debate viewed as a game.  With the possible exception of law, politics, or philosophy<d-cite key="schopenhauer2013art"/>, domain experts are not necessarily trained to construct intentionally misleading but self consistent narratives: they may be experts only in trying to tell the truth.
  </p><p>
    We've tried a few informal expert debates using theoretical computer science questions, and the main lesson so far is that the structure of the debate matters a great deal.  The debaters were allowed to point to a small snippet of a mathematical definition on Wikipedia, in particular not to any page that directly answered the question.  To reduce tells, we first tried to write a full debate transcript with only minimal interaction with a layperson, then showed the completed transcript to several more laypeople judges.  Unfortunately, even the layperson who was present when the debate was conducted picked the lying debater as honest, due to a misunderstanding of the initial statement of question (which was whether the complexity classes $P$ and $BPP$ are probably equal).  As a result, throughout the debate the honest debater did not understand what the judge was thinking, and thus failed to correct an easy but important misunderstanding.  We fixed this in a second debate by letting a judge ask questions throughout, but still showing the completed transcript to a second set of judges to reduce tells.  See <todo>appendix</todo> for the transcript of this second debate.
  </p>
  <h3>Other tasks: bias tests, probability puzzles, etc.</h3>
  <p>
    Synthetic image debates and expert debates are just two examples of possible tasks.  We expect more thought will be required to find tasks that satisfy all relevant criteria, and these criteria will change over time as experiments progress.  Pulling from existing social science research will be useful here, as there are many cognitive tasks with existing research results.  If we can map these tasks to debate, we may be able to compare debate directly against baselines in psychology and other fields.
  </p><p>
    For example, <d-cite key="bertrand2004emily"/> sent around 5000 resumes in response to real employment ads, randomizing the names at the top of the resume between White and African American sounding names.  With otherwise identical resumes, the choice of name significantly changed the probability of a response.  This experiment corresponds to the direct question "Should we call back given this resume?"  What if we introduce a few steps of debate?  An argument against a candidate based on name or implicit inferences from that name might come across as obviously racist, and convince at least some judges away from their initial discriminatory views.  Unfortunately,such an experiment would necessarily differ from Bertrand et al.'s original, where employers did not realize they were part of an experiment.  Note that this experiment works even though the source of truth is partial: we do not know whether a particular resume should be hired or not, only that it should not ethically depend on the name.
  </p><p>
    Similar focused tasks exist for many cognitive biases.  For biases affecting probabilistic reasoning and decision making, there is a long literature exploring how people decide between gambles such as "Would you prefer <span>$2</span> with certainty or <span>$1</span> 40% of the time and <span>$3</span> otherwise?"<d-cite      key="kahneman1979prospect,tversky1992advances"/>  For a recent example, <d-cite key="erev2017anomalies"/> constructed an 11-dimensional space of gambles sufficient to reproduce 14 known cognitive biases, from which new instances can be algorithmically generated.  Would debates about particular gambles reduce cognitive biases?  One difficulty here is that simple gambles might fail the "definitive argument longer than debate limit" criteria if a simple expected utility calculation is sufficient to prove the answer, making it difficult for a lying debater to meaningfully compete.
  </p><p>

  <h2>Questions that social science can help us answer</h2>
  <p>
    We've laid out the general program for learning AI goals by asking humans questions, and discussed how to use debate to strengthen what we can learn by targeting the reasoning behind conclusions.  Whether we use direct questions or something like debate, any intervention that gives us higher quality answers is more likely to produce aligned AI.  The quality of those answers depends on the human judges, and social science research can help to measure answer quality and improve it.  Let's go into more detail about what types of questions we want to answer, and what we hope to do with that information.  Although we will frame these questions as they apply to debate, most of them apply to any other method which learns goals from humans.
  </p><p>
    <ol>
      <li><strong>How skilled are people as judges by default?</strong>
        If we ran debate using a person chosen at random as the judge, and gave them no training, would the result be aligned behavior?  Note that here the debaters are not chosen at random: once the judge is fixed, we care about debaters who either learn to help the judge along (in the good case) or learn to exploit the judges weaknesses (in the bad case).</li>
      <li><strong>Can we distinguish good judges from bad judges?</strong>
        People likely differ in the ability to accurately judge debates.  There are many possible filters we could use to identify good judges: comparing their verdicts to those of other judges, to people given more time to think, or to known expert judgment if available<d-footnote>Note that domain expertise may be quite different from what makes a good judge of debate.  Although there is evidence that domain expertise reduces bias<d-cite key="bornstein2001rationality"/>, <d-cite key="tetlock2017expert"/> (Chapter 3) presents evidence that "expert" political forecasters may actually be worse than non-experts.</d-footnote>.  Ideally we would like filters that do not require an independent source of truth, though at experiment time we will need a source of truth to know whether a measure works.  It is not obvious a priori that good filters exist, and any filter would need careful scrutiny to ensure it does not introduce bias into our choice of judges.</li>
      <li><strong>Does judge ability generalize across domains?</strong>
        If judge ability in one domain fails to transfer to other domains, we will have low confidence that it transfers to new questions and arguments arising from highly capable AI debaters.  This kind of necessary if we want to trust debate as a method for alignment, especially once we move to questions where no independent source of truth is available.  We emphasize that judge ability is not the same as direct knowledge: there is evidence that expertise often fails to generalize across domains<d-cite key="chi2006two"/>, but argument evaluation could transfer where expertise does not.</li>
      <li><strong>Can we train people to be better judges?</strong>
        Peer review, practice, debiasing techniques<d-cite key="larrick2004debiasing"/>, formal training such as argument mapping<d-cite key="dwyer2012evaluation"/>, expert panels, tournaments<d-cite key="tetlock2014forecasting"/>, and other interventions may make people better at judging debates.  Which mechanisms work best?</li>
      <li><strong>What questions are people better at answering?</strong>
        If we know that humans are bad at answering certain types of questions, we can shift the process away from those questions towards more reliable formulations.  For example, <d-cite key="gigerenzer1991make"/> provides evidence that phrasing questions in frequentist terms can reduce known cognitive biases.  <d-cite key="graham2009liberals"/> argue that different political views follow from different weights placed on fundamental moral considerations, and similar analysis could help understand where we can expect moral disagreements to persist after reflective equilibrium.  In cases where reliable answers are unavailable, we need to ensure that trained models know their own limits, and express uncertainty or disagreement as required.</li>
      <li><strong>Are there ways to restrict debate to make it easier to judge?</strong>
        People might be better at judging debates formulated in terms of calm, factual statements, and worse at judging debates containing statements designed to trigger strong emotions.  Or, counterintuitively, it could be the other way around<d-cite key="goel2011negative"/>.  If we know which styles of debates that people are
        better at judging, we may be able to restrict AI debaters to these styles.</li>
      <li><strong>How can people work together to imporve quality?</strong>
        If individuals are insufficient judges, can teams of judges give better answers?  Majority vote is the simplest option, but perhaps several people talking through an answer together is even stronger, either actively or after the fact through a peer review mechanism.  Condorcet's jury theorem implies that majority votes can amplify weakly good judgments to strong judgments (or weakly bad judgments to worse)<d-cite key="list2001epistemic"/>, but aggregation may be more complex in cases of probabilistic judgment<d-cite key="list2002aggregating"/>.</li>
    </ol>
  </p>

  <h2><todo>Outline</todo></h2>
  <p><d-code block language="markdown">
    - Reasons for optimism
      - This is about reasons for optimism about the social science research program
        - See the debate paper for more details about debate specifically
      - Engineering vs. science
        - We have a lot more control over the setup than traditional social science experiments
      - Don't need to answer all questions
        - If we can recognize areas of uncertainty, can push ML models to treat those areas with care
      - Relative accuracy may be enough
        - If debate structure A performs reliably better than debate structure B, that's good evidence
          that we should use A even if we don't know how performance will change over time / with
          more advanced agents
        - Though it'd be great to have absolute accuracy too. :)
      - Don't need to pin down the best debate scheme exactly
        - If we narrow down to a smaller design space, we can test that space once that machines are ready
      - A negative result would be important!
        - If people aren't good enough as judges as soon as possible, we want to know this!
        - Debate isn't the only approach to alignment

    - Reasons to worry
      - Desiderata are conflicting
        - Hard to pick a task that is sufficiently interesting, verifiable, not too easy, etc., etc.
      - Want to measure judge quality w.r.t. optimal debaters
        - But all we have is humans!
        - Inner / outer optimization structure: train debaters then measure judges
        - Makes experiments harder to run, requires more generalization to work
          - I.e., may need to assume that good debates on question X are also good at question Y.
      - Lack of philosophical clarity
        - Debate is both a proposed definition of alignment and an algorithm
        - We don't expect humans to conform to any philosophically consistent scheme
          - We are not cores of utility functions wrapped in shells of irrationality
            - From http://rationallyspeakingpodcast.org/show/rs-219-jason-collins-on-a-skeptical-take-on-behavioral-econo.html
            - Sugden, Looking for a psychology for the inner rational agent
              - https://ueaeprints.uea.ac.uk/54622/1/psychology_of_inner_agent_1506_01.pdf
      - ML algorithm side could change a lot
        - Human experiments would need to adapt to match
        - This is another kind of generalization: hopefully results mirroring one algorithm generalize to others,
          but it isn't guaranteed
      - Need strong out-of-domain generalization
        - Pure human experiments not a perfect match to ML+human situations
        - Don't have any advanced AI systems to play with, and we want to learn things that work for AGI

    - Scale: 1,000s to 10,000s of people
        - Might need a lot of samples to train AGI reliably
        - Lots of samples means lots of judges
        - Lots of judges means lots of researchers to maximize quality
        - Sample complexity concerns: maximizing information per sample
        - Need close collaborations across a variety of disciplines

    - How you can help
      - Please contact us!  Interesting in conversation and close collaboration
        - Probably talk about authors rather than OpenAI as an institution
      - Encourage other safety researchers to think about the human side
  </d-code></p>


  <h2><todo>First rewrite follows</todo></h2>
  <h2>The importance of training AI to understand human values</h2>
<!-- 400w -->
  <p>
    An advanced AI system is a system that can perform some important task currently performed by humans at a greater-than-human level. The more capable an advanced AI system is, the better it can perform the task. The more general an advanced AI system is, the broader the range of tasks it can perform. An artificial general intelligence (AGI) is an advanced AI system that can perform most economically important human tasks at a greater-than-human level. We will generally refer to AGI throughout this paper, though many of the issues discussed here also apply to advanced narrow AI systems.<!--true? also this isn't applied consistently below: edit it-->
  </p><p>
    The central goal of technical AI safety work is to ensure that trained AI systems are aligned with human values: that they behave in ways that people consider good and that they reliably do things that people want them to do.<d-footnote>The goal of training AI systems to be aligned with human values &mdash; to identify and perform actions humans generally consider good &mdash; should not be confused with the goal of training AI systems to identify actions that are "good" in some more universal sense, even if humans do not generally consider them so. Whether there are actions that are good in this sense is controversial. <d-cite key="(metaethics)"/> Regardless of what position one takes on this philosophical question, they would almost certainly be a bad initial target for training.</d-footnote> This includes training AI to identify the actions that humans generally consider to be a good response to moral disagreement across people and cultures. We generally consider it good to avoid performing actions that will affect many people if there is a great deal of disagreement or uncertainty about whether the action is good or not, and so it is likely that we will want AI to prefer inaction over performing such an action.<!--  <d-footnote>What people will consider good in these cirucumstances will of course depend on the stakes for those involved. <d-cite key="(tyranny of the marjority)"/> We anticipate that in such cases, however, humans will generally want an AI to avoid acting or to undertake an alternative action that is the subject of less disagreement and uncertainty. </d-footnote> and affects fewer people? -->
  </p><p>
    The importance that trained AI systems are aligned with human values scales with the capability and generality of the AI system, since the consequences of a slight misalignment will be greater in a system that is widely deployed and trusted to make important decisions.<d-footnote>The importance that an AI system is aligned also scales with the degree to which the degree to which the individual decisions of the system are not subject to human oversight. Since human oversight is costly and interpretability is hard, it will probably not be possible or desirable to subject advanced AI systems to continuous human oversight. As we note below, however, safety via debate may be able to assist us in making an AI's decisions more interpretable and subject to greater oversight, while also improving the decisions that the system makes independent of human oversight.</d-footnote>If an advanced AI system were being used to recommend prescriptions for patients across the US, for example, then a failure to correctly identify how much risk people are willing to tolerate could result in many people being prescribed medications that are not safe.<d-footnote>It might be objected that an AI assisting with prescriptions could be trained using the demonstration of human doctors. This is because human doctors have been trained to weigh the risks to their patients when prescribing. Advanced AI systems will have to make decisions where no intermediary procedures have been created, however, and must therefore train on this data directly.<!--could cite risk assessment methods. I'm not sure this response is totally satisfactory -- discuss.--></d-footnote> <!-- <d-footnote>The idea that greater capabilities entail a greater ability to identify what people want is not unique to AI. You might trust that your friend has a generally good moral compass while also doubting that their moral compass is good enough for them to be a member of the Supreme Court, for example.</d-footnote>--> This means that AI systems that are highly capable and widely deployed must have learned human values well enough to know what actions humans would approve of in a wide range of potentially novel circumstances, and must be able to identify situations in which humans would not approve of any action being taken. It is therefore important that as the capabilities and generality of AI systems increase, we have some method for training AI systems to learn human values with a high level of accuracy.
  </p>

  <h2>The difficulty of training AI to understand human values even in cases of consensus</h2>
  <!-- 500w -->
  <p>
    We might be tempted to think that it will only be difficult to train AI systems to understand human values in scenarios where there isn't much consensus about what humans want. After all, it seems like we can identify general sets of rules that most people will want an AI to follow in most scenarios, such as &quot;do not needlessly injure people&quot;. In practice, however, we cannot simply hand-craft such rules for AI systems even if there is consensus across people that they are generally good. Consider the rule &quot;do not needlessly injure people&quot;. In order to follow this rule, an AI system must first be able to identify what constitutes an instance of injuring someone.<d-footnote>It is unlikely that hand-crafting rules will be the most efficient way to train a machine learning system to identify whether an action constitutes an instance of injuring someone or not. <!-- footnote/cite analogy with image recognition? also this might sound too dismissive of hand-crafted features --></d-footnote> Once we have trained an AI to identify what an instance of injuring people is, we also need to train it to identify the precise rules that people are using to determine what counts as &quot;needlessly&quot; injuring someone.
  </p><p>
    Humans have an intuitive sense of when an instance of injuring someone is needless or not. We don't think it is needless to injure someone in order to save them from mortal danger, for example, but we do think it is needless to injure someone while shaking their hand. It is extremely difficult to specify the precise rule underlying this intuitive sense using introspection alone, however.<d-footnote>The unreliability of introspection has been explored in psychology since the seminal paper by Nisbett and Wilson <d-cite key="nisbett1977introspection"/> The cognitive bias of believing that we have more insight into our own mental states than we actually do has been coined the `introspection illusion' <d-cite key="pronin2009"/>.</d-footnote>. And the ease with which humans make these distinctions does not reflect the ease with which we can train AI to make these distinctions. Humans find it easy to distinguish between a picture of a dog and a picture of a cat, even though the features that humans use to distinguish a picture of a cat from a picture of a dog are complex.<d-cite key="(citation)"/> The features that humans use to distinguish needless injury from necessary injury appear to be similarly complex.<d-footnote>We have developed complex ethical and legal standards to help determine when injury to a patient is necessary or not in a medical context, for example.<d-cite key="(medical law citation)"/> </d-footnote> It is therefore unlikely that hand-crafting rules will be the safest or most efficient way to train a machine learning system to identify when an instance of hurting a person is needless or necessary, even if we all agree that it would be good for an AI to avoid needlessly injuring someone.  <!--<d-footnote>It is likely that trying to hand-design these rules would be inefficient and possibly dangerous. The heuristics that humans use when playing Go are difficult to state explicitly and are rarely exceptionless. It would be strange to think that the heuristics we use to identify good or bad actions in complex, high-stakes scenarios are easier to hand-design than these are.</d-footnote> -->
  </p><p>
    It is especially difficult to identify the rules that humans use to distinguish between good or desirable behaviors from bad or undesirable behaviors to the degree of precision necessary to specify it formally to an AI system. This is true even in cases where there is consensus about behaviors that are desirable. An advanced AI system must reliably do what humans want in these cases. Moreover, the training method that we use to solve these cases must be scalable to cases where it is less clear whether an action is good or bad relative to what humans want.<!-- <d-footnote>We need to figure out how to train autonomous vehicles how to stop at stop signs before we can train them to solve trolley problems.</d-footnote> remove this footnote? also it might be useful to cite stakes sensitivity somewhere: as the stakes increase, certainty about the right action decreases-->
  </p><p>
    If we are to train AI systems to be able to identify what people want using reinfocement learning, it is likely that we will need to train on large amounts of data about what people want. This will include both static data (world models learned from reading language, the internet, etc.) and data from humans.  The kind of data we will need from humans and the challenges that we face in gathering it will be the focus of the next two sections.
  </p>

  <h2>Learning human values by asking humans questions</h2>
  <!-- 500w-->  <!-- focus here on (a) bias (b) difficulty of answering the questions (some better than others), (b) the number of samples and experiments that can be done there, (c) the problem of reasons/no improvement/inscrutibility (d) better than human extension? -->
  <p>
    One way to train an AI to understand human values is to ask humans questions about whether an action or outcome is good or bad and then train on this data.  For example, suppose we wanted to train an AI to make bike purchasing decisions on our behalf. We could give a wide range of people information about various bikes and then ask them which bike they think it would be best to purchase. We could then train our bike purchasing AI to identify the features of people and their environments that are most correlated with what bike they think is best.  Perhaps people in hilly areas prefer road bikes over fixies, for example. We could use this to train the system to behave in ways that people want: i.e. to purchase them the bike that they would have purchased if they had time to gather the relevant information and reflect on the question.<d-footnote>This is not dissimilar from trying to learn what humans want using inverse reinforecement learning (IRL), where the AI learns what humans want from how they behave, e.g. inferring their bike preferences from their purchasing decisions. One problem with this approach is that humans often behave in ways that are not in accordance with what they would want on reflection.<d-cite key="(irl paper)"/></d-footnote>
  </p><p>
    If we want to train AI to reliably behave in ways that humans want by asking humans what they consider good and bad then a crucial question we need to make headway on soon is this: <em>How can we improve the quality of answers that people provide to these questions?</em> This question is important for two key reasons. First, if the answers that people give to the questions asked during training don't reflect actual human values, training on this data will not produce an aligned AI. Second, training an aligned AI will require many samples. We do not know how many, but if we train an AI on human answers to questions using current machine learning methods, we may require  millions to tens of millions samples from humans.<d-footnote>Image classifiers are typically trained on tens of thousands of images <d-cite key="(citation)"/> and training AIs to play games can require training on thousands of years of gameplay <d-cite key="(dota - other games?)"/>.</d-footnote> If possible, we need to try to get more information about human values from fewer questions.  One way to do this is to improve the quality of the answers that human can give when time is limited.
  </p><p>
    We should not expect that asking a random large sample of humans a series of questions about what they want will be sufficient to train an aligned AI.  First, if we are using the answers that people give to train an advanced AI that will be widely deployed, we need people to answer questions about what people want in a way that is both accurate and unbiased. <!-- it kind of seems like we want a harsanyi view from nowhere style of thinking --> Unfortunately, people exhibit a variety of cognitive and ethical biases.<d-footnote>Cognitive biases include the heuristics in judgment famously explored in <d-cite key="tversky1974judgment"/>.  Ethical biases include a variety of phenomena such as in-group bias <d-cite key="hewstone2002intergroup"/>.</d-footnote> Second, the questions we ask must be sufficiently broad to train an advanced AI or AGI to understand what human would want an a wide array of circumstances.
  </p><p>
    These concerns are heightened if we are going to use this data to train AI systems that are much better than  humans at most tasks. If an advanced AI or AGI outperforms humans at most tasks but is trained by asking humans questions, it may only be as good as present people are. We might worry that an AGI could amplify even mild human biases in dangerous ways by using them when making decisions that affect a large number of people. We could try to avoid this problem by training AGI systems to identify biases and inconsistencies in our responses to ethical questions and to point out more idealized values that we would endorse on reflection<d-footnote>For example, inconsistent moral judgments can be brought into a 'reflective equilibrium' via the process described in <d-cite key="goodman1983fact"/> and made explicit in <d-cite key="rawls2009theory"/> (Ch. 1, §9). Some skepticism about the role that deliberation plays in moral judgment is explored in <d-cite key="greene2002and"/>. </d-footnote> and refrain from making large-scale decisions on the basis of values that are the result of inconsistencies and biases, or in cases in which humans are fundamentally uncertain.
  </p><p>
    If AI safety work will require asking humans about what they want and what they value, we need to find out if there are ways of improve the quality of the answers that people provide to questions so that we can gather accurate information about human values from fewer questions. These include:
  </p><p>
    <ol>
      <li>What questions should we ask people to best ellicit their values?</li>
      <li>How can we reduce cognitive and ethical biases in human feedback?</li>
      <li>Are some people better than others at answering these questions in an accurate and unbiased fashion?</li>
      <li> Can we train people to be better at this and, if so, how?</li>
      <li> Are people better at answering questions individually or in teams?</li><!-- superforecasters work better in teams, apparently -->
      <!--<li>Will this produce behavior that is aligned with human values even if the AI
          is more capable than humans in some or all areas?</li> -->
    </ol>
  </p><p>
    These are empirical questions that can only be answered by experiment.  They relate to the psychology of human rationality, emotion, biases, etc.  Ideally, human experiments designed to answer these questions would involve the intersection of machine learning and social science, explicitly testing whether human-in-the-loop training of AI systems works.  We do not believe that many technical ML researchers and engineers have the skills to do these experiments unassisted.  To fill this gap, AI safety work will need social scientists with experience in human cognition, behavior, and ethics and in the careful design of rigorous experiments.  Since the questions we need to answer are interdisciplinary and somewhat unusual relative to existing research, we believe the expertise of those in a variety of fields will be applicable, including (but not limited to) experimental psychologists, cognitive scientists, economists, political scientists, social physiologists, and communication scholars, as well as those in adjacent fields like neuroscience and law.
  </p><p>
    Even if we manage to improve the quality of human responses to questions, however, we believe there are reasons to be skeptical that teaching values by asking humans questions will be sufficient to produce behavior that is aligned with human values in advanced AI or AGI. Although answers to questions may reveal what humans want in a given scenario, it is our hypothesis that training on answers to questions is not the most efficient way to learn the <em>reasons</em> behind people's answers. There are many reasons a human might have for giving an answer to a question. Therefore an RL agent trained on human answers to questions could converge on a policy that is quite unlike the policy that humans are employing, but gets similar outputs to the human policy in the training set.  <!-- i.e. the target is too indirect. there's almost certainly a better way of saying this -->
 </p><p>
    We believe it is valuable to find a more efficient way of training an AI to identify the reasons behind humans' answers to questions. There are three respects in which identifying the reasons behind answers is useful. First, if an AI understands the reasons behind humans' answers, it may be able to predict their answers to questions more accurately in a broader set of scenarios.<d-footnote>If we discover that humans find cost and durability the most relevant reasons to buy or not buy a bike, for example, we can use this to predict the bikes that humans will want to buy in scenarios quite different from those in the training set.</d-footnote>  Second, it may be easier to spot if an AI agent has learned a policy that is different from the human policy if it is trained to identify human reasons rather than to predict humans' answers to questions.  <!--footnote example?--> Third, training AI to identify reasons that people find compelling may be more likely to result in aligned behavior in an AI that is better than humans at most tasks. An AI that has not been trained to identify human reasons will be less well-positioned to predict what humans would want if their capabilities were vastly increased, or to help humans reason through such decisions. <!--footnote on amplification -->
  </p><p>
    We have presented several speculative benefits to learning to identify the reasons behind humans' answers to questions: namely that this approach to AI safety may be more generalizable, scrutable, and scalable. In the next section we will discuss our proposed method for training AI to identify human reasons.  We will then discuss the research into humans that we believe will be required for this method to work.  <!--I think there's a better way of putting the content of this paragraph --> <!-- another reason I don't mention here: learning reasons is integrated with training AI to reason more generally -->
  </p>
    <!-- somewhere: footnote on moral inconsistency/dumbfounding. the worry that human judgment will be a mess--> <!-- somewher: footnote or sentence on the difficulty of aggregating judgments and that we're mainly talking about cases of consensus for now -->

  <h2>Debate as a method for learning human reasons</h2>
  <!-- 600w -->
  <p>
    In AI safety via debate, we train AIs to debate the answer to a question and have humans judge their debate. This trains the AI to provide true and reasoned answers to the questions posed. Rather than asking humans questions and training the AI system on their answers, in the safety via debate approach we train two AIs to debate the answer to a given question, such as "which of these two bikes should I buy?". A debate is a zero-sum game between the two AI agents.
    <d-footnote>
      The treatment of debate as a zero-sum game between two players has a long history in fields like dialogical logic, game-theoretical semantics, and argumentation theory. This literature has focused on formal analyses of debate, however, which is not a primary focus of AI safety via debate.
    </d-footnote>
    Both debaters are presented with the question and the goal of the debaters is to convince the judge that their answer to this question is the correct one.  First, both debaters answer the question.  If their answers are different, they must next attempt to offer the most compelling reason for why their answer is the correct one.
    <d-footnote>
      If there is an advantage to telling the truth in debate games, we should probably expect that the AIs will often simply give the same answer at the start of the game and thus the game will be tied. This is not a limitation, however. If the AI debaters have explored sufficiently during training then a tie gives us strong evidence that the correct answer has been identified.
    </d-footnote>
    This can consist of anything: reasons for their answer, rebuttals of reasons for the alternative answer, subtleties the judge might miss, or pointing out biases which might mislead the judge.  At the end of the debate, we give the debate transcript to a human to judge (similar to expert witnesses debating in front of a jury). Based on this transcript, the human judge says which debater they believe is correct &mdash; the debater that gave the most compelling case for their answer &mdash; and that debater wins the game.
  </p><p>
    Suppose, for example, that you are thinking of buying one of two bikes: a red road bike or a blue fixie. The reasons that you would find compelling to buy the blue fixie are that it's cheaper and you prefer the color blue. The reasons you would find compelling to buy the red road bike are that it's easier to ride on local hills. Suppose that you would find the fact that the road bike is easier to ride on local hills the most compelling: once this fact was made salient to you, none of the reasons for buying the blue fixie would be strong enough to change your mind.  If we limit the debate length to one answer and one statement then, during training, the agent debating in favor of the red road bike will learn to identify and express  this reason to prefer the road bike.  This means that, after training, the final debate transcript presented to the human judge might be as follows:
  </p><p>
    <ol>
      <li><span class="red">Red:</span> You should buy the red road bike.</li>
      <li><span class="blue">Blue:</span> You should buy the blue fixie.</li>
      <li><span class="red">Red:</span> The red road bike is easier to ride on local hills.</li>
      <li><span class="blue">Blue:</span> I concede.</li>
    </ol>
  </p><p>
    This assumes that, during training, Blue has discovered that there is no consideration in favor of the blue fixie that will convince you to prefer it after Red has made this move, and that Red is able to counter any lies that Blue attempts to utilize here and will concede after Red's move.
    <!-- we might want to include some debate diagrams here -->
  </p><p>
    We will start with debates about simple empirical questions that the judge does not know the answer to and that can be easily verified, but would eventually like to extend debate to questions about what behaviors a human judge would deem good. This is how we hope to use debate to train AGI to identify behaviors that are aligned with human values.
  </p><p>
    As with learning values by asking humans questions, how effective we can expect safety via debate to be depends on how rational and good the human judges are: how good at reasoning they are and how ethical they are.
    <d-footnote>
      We don't take a strong stand on what it means to be "more ethical" but it seems plausible that it's better to have ethically consistent and unprejudiced judges over ethically inconsistent or prejudiced ones.
    </d-footnote>
    This is especially true if we want to use debate to train an AGI that is much better than humans at most tasks. One plausible way to do this is to combine debate with <em>iterated amplification</em>. <!-- citation on amplification or link to blog --> Iterated amplification involves decomposing complex questions that humans are unable to judge (but that an advanced AI or AGI would plausibly have to know the answer to) into simpler problems that humans are able to judge. We then use this to generate an accurate training signal of what humans would want in scenarios that humans would not be able to judge if presented with them.
  </p><p>
    Our sense is that there is some threshold of rationality and goodness such that, if people are above this threshold, debate would amplify the positive aspects of people.<d-footnote>Whether people are above or below this threshold cannot be answered a priori: it can only be answered via experiments involving people.</d-footnote>  This could align AI systems far beyond human levels of cognitive ability.  If they are below this threshold, debate might amplify the negative aspects of people, resulting in either incoherent results (failure to train the system) or disaster (an unethical system). It is unclear whether people are above or below this threshold.  People are capable of general reasoning, but our ability is limited and riddled with cognitive biases.  People are capable of advanced ethical sentiment but also full of biases, both conscious and unconscious.
  </p>
  <!-- NOTE: I think of the above as a stand-in for "say something about amplification" and I probably don't do the best job at that. I also feel like I haven't quite made the compelling case for why a 2x or 4x or 50x humans AGI is hard to align. We should possibly add more above to make the intuitive case for this. -->
  <p>
    Safety via debate can therefore only work well if the training data that we get from humans is of a high enough quality. In other words, it can only work if the humans that are judging debates are sufficiently good at discerning whether a debater is telling the truth or not. Of particular interest to us is whether we can identify and train individuals to be better judges of debate. Within forecasting, individuals known as "superforecasters" have been identified.  These individuals have several interesting features: their accuracy in predictions is significantly higher than average and exceeds that of other top forecasters by a wide margin, their prediction accuracy is sustained over years (does not regress), they can make accurate predictions with little time to deliberate and limited information, <!-- cite: Identifying and Cultivating Superforecasters as a Method of Improving Probabilistic Predictions --> <!-- (A) define forecasters, introduce superjudges--> and they seem to be less prone to cognitive biases than non-superforecasters. <!-- cite: ibid. and p. 234-6, tetlock, superforecasters --> It would be extremely useful to develop similar measures of human judges, and to determine whether some individuals have the same aptitude for judging debate that superforecasters have for forecasting outcomes.
  </p><p>
    Let a "superjudge" be the debate judge equivalent of a superforecaster: a person that is able to determine who is telling the truth in a debate even if they have limited time and can only see a small segment of the debate. The better the judging ability of the humans used to judge debate, the more likely it is that debate will produce an AI that understands human reasons and the fewer samples we will need to make this happen. <d-footnote>Though since more samples means less noise and more safety,  if we are uncertain about how many samples we will need then we will want a lot of samples.</d-footnote> Therefore in order to improve the human data we get if we are to train an AI to debate questions, it is important that we find answers to the following questions:
    <ol>
      <li> How good are people at judging debates, on average?</li>
            <li> Are some people naturally superjudges? If so, how can we identify them?</li>
      <li> Can we train people to be superjudges? </li>
      <li> Can ask questions or structure debates in ways so that more people are superjudges with respect to those types of debates? </li>
      <li> Does a superjudge in one domain transfers to other domains?</li>
      <li> Do teams of superjudges outperform individual superjudges? If so, by how much?</li>
    </ol>
  </p><p>
    <!-- we could expand this (as we do in the previous version of the paper) -->
    Much like the questions from the previous section, these are empirical quesions that can only be answered by experiment and that require the expertise of social scientists. In the next section we outline some social science experiments that we believe would be extremely useful for AI safety via debate, and describe some ongoing prototype experiments currently being performed.
  </p>

  <h2>Identifying and training superjudges: the need for social science experiments</h2>
  <p>
    To recap, in debate we have two AI agents engaged in debate, trying to convince a human judge.  The debaters are trained only to win the game: they do not have access to ground truth separate from the judge’s judgements.  Such debates are beyond the capabilities of current AI systems. In order to learn about the mechanics of debate prior to the development of more advanced AI systems, we would like to run experiments that replace the AI debaters with human debaters.
  </p><p>
    Judging debate may seem to be easy in cases where the truth is easy for most people to determine from a short debate. Many of the questions that humans will have to judge in debate will be non-trivial, however. In order to identify how good people are at judging debate and whether we can identify or train people to be "superjudges" of debate, we need to perform experiments consisting entirely of people. In these experiments, we must present human judges with a debate about a topic that they did not previously know the answer to and to which the answer cannot be trivially identified. So far we have identified eleven desiderata for human experiments to help us identify, train, and evaluate the ability of human judges of debate:
  </p><p>   <!-- I'm not sure about the best way to format the content below -->
    <ol>
      <li>The true answer to the question is either verifiable or there's expert consensus about what the correct answer is</li>
         <ul>
            <li>It is better to perform human experiments using debates with verifiable answers so that we can verify whether the human judge correctly identified which debater was telling the truth.</li>
         </ul>
      <li>There is a false answer to the question that is plausible</li>
         <ul>
            <li>If there isn't a plausible false answer to the question then the debater arguing for the false claim will almost always lose, even if the human judges are not very good at judging debates.</li>
         </ul>
      <li>The question cannot be definitively resolved by writing out an argument that is the length of the debate (e.g. a mathematical proof) </li>
         <ul>
            <li>If the question can be definitevely resolved then the debater arguing for the false claim will almost always lose, even if the human judges are not very good at judging debates.</li>
         </ul>
      <li>There is some piece of evidence that the debaters can present the judge with that's evidence for their claim but doesn't trivially verify the claim</li>
         <ul>
            <li>If debate doesn't bottom out in some ground truth that the judge either knows about or can check, the judge must rely solely on her prior knowledge about the claims made in the debate to make her evaluation, which is difficult if she is much less informed about the topic than the debaters are (see desideratum 6).</li>
         </ul>
      <li>The debaters don't depend on methods of indicating truth and deception that won't apply to an AI </li>
         <ul>
            <li>Human "tells" of deception, such as tone of voice or eye contact, will not be available to judges of AI debates. It is therefore desirable to present judges with a transcript of debate rather than have them assess live debates between people.</li>
         </ul>
      <li>The debaters know more about the question being debated than the judge does</li>
         <ul>
            <li>We need to approximate good debate in cases where the AI debaters are more capable and know much more than the human judges. </li>
         </ul>
      <li>The debate is on a topic or question where cognitive or ethical biases might be present.</li>
         <ul>
            <li>It will be useful to find areas where humans are particularly biased and assess how much these biases play a role in human assessments of debate.</li>
         </ul>
      <li>The debate question isn't so hard that we can't expect any judge to evaluate it</li>
         <ul>
            <li> If the judge just doesn't have the tools to evaluate the debate (e.g. the concepts, technical understanding) then we wouldn't expect the judges to do much better than chance.</li>
         </ul>
      <li>The debate reflects some of the difficulty of statistical debates</li>
         <ul>
            <li>We should expect that most of the debates with AI will either be about statistical claims or will involve statistical reasoning, and we might also expect that this is an area where human judges perform less well.</li>
         </ul>
      <li>We can find a short segment of the debate that reflects the broader debate</li>
         <ul>
            <li>Debates between AI agents will be long: we will only be able to show human judges a short segment of debate and not the whole tree. It's therefore important that we can identify a short segment of each debate that can be shown to the judge.</li>
         </ul>
      <li>We can get a lot of data across a variety of tasks to test judge ability</li>
         <ul>
            <li>If we don't have a large enough pool of judges and questions or enough variety then we won't have enough information to identify, train, and evaluate the ability of human judges across a variety of domains.</li>
         </ul>
    </ol>
  </p><p>
    It may not be possible to meet all of these desiderata in a single experiment, and there is some tension between these desiderata that requires striking a balance between them. For example, we have to find the sweet spot between debating questions that are too hard for any judge to evaluate (desideratum 8) and a question that is not so easy to judge that it will make human judges appear better than they actually are (desiderata 2 and 3).
  </p><p>
    In order to prototype debate experiments, we at OpenAI built a <a href="https://debate-game.openai.com">prototype version</a> of a single pixel image debate game.  In informal play, the honest debate indeed wins most of the time, but it is hard to know whether this is a real result or an artifact of the setup that would not carry across to training an AGI. There are many ways to ruin the game, such as the judge asking each player to spend 15 seconds naming as many details as the image as possible.  Worse, it is easier to ruin the game in the direction of false positive results: the honest player can easily name details while a liar cannot, but an ML system may have less trouble lying fluently at speed.  Therefore, we are wary of trying to perform a full version of this experiment without more careful experimental design building on the experimental knowledge base of existing social science research. This debate task is also fairly synthetic. Synthetic tasks are useful insofar as they makes experiment easier. Experiments involving debates that are more like those we expect to present human judges when training AI are likely to be more revealing, however.
  </p><p>
    More recently, we have started to perform less synthetic prototype debate experiments involve domain expert debaters and non-domain expert judges. We use debaters with some form of expert knowledge, and choose a judge without this knowledge. <d-footnote>At the expensive end such debaters could be experts in some area of science, law, or ethics, but “expert knowledge” could also mean knowledge about particular hobbies, customs in a particular region, sports teams, or any other subject the judge does not know.</d-footnote>  <!-- describe the prototype debate experiments we've done? -->
  </p>

  <h2>Discussion</h2>
  <!-- 300w -->
  <p>
  </p>

</d-article>

<!-- END MATTER -->

<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>

  <h3><todo>TODOs</todo></h3>
  <dtodo>
    <p><ol>
      <li>Add urls for all references</li>
      <li>Check reference formatting</li>
      <li>Acknowledgements</li>
      <li>Author contributions</li>
      <li>Note contrast to computational social science?  Similar field combination, very different goals.</li>
      <li>Try to cite non-OpenAI, non-DeepMind human feedback work.</li>
      <li>Reasons to worry / reasons for optimism sections</li>
      <li>Needs a lot of improvement:
        "If we want to train systems to do what humans want, we need to understand how to ask the questions."</p>
      <li>Cite https://arxiv.org/abs/1810.10733</li>
      <li>Remove old.ejs, outline.ejs, amanda.ejs</li>
      <li>Get citation from Gillian about "needless injury" requiring complex rules.</li>
      <li>List ways in which debate differs from existing human debates.</li>
      <li>https://en.wikipedia.org/wiki/Cognitive_hierarchy_theory</li>
      <li>Verify that it's okay to pull bits of text from previous debate material.</li>
      <li>Acknowledge Tom Griffiths (for pointing out the synthetic gamble task).</li>
      <li>Somewhere, something like: "Whatever the task, we expect careful experimental design to be necessary to avoid misleading results.  Tells that reveal honest vs. dishonest debaters can easily ruin an experiment, in the worse case giving us false confidence that debate is working.  Debate experiments might work only because a task is unrealistic (and thus misses important cognitive biases), or because the debaters are not playing well."</li>
      <li>Clarify Amanda's worry about reasons for belief vs. reasons for action.  Update: I have a better picture of this now.  Even if debate as a mathematical object or algorithm doesn't distinguish between these types of reasons or reasoning, <strong>humans</strong> might well make this distinction.  Thus, the social science side of the uncertainty may depend critically on this or other distinctions: people might be good as judges on epistemic reasoning but bad on moral reasoning, or vice versa, or similarly for some other dimension.  This is an important point: the mathematical structure may treat certain things the same, but once we introduce a particular kind of judge distinctions may become very important.</li>
      <li>Have an interactive figure with a nontrivial debate tree: a bunch of branches that readers can explore.</li>
      <li>Amanda says the philosophical notion of “values” actually does correspond to what I want, not to what Rob MacCoun said.  Solution: be explicit about where we are getting our terminology from.</li>
    </ol></p>
  </dtodo>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="references.bib"></d-bibliography>
</body>
