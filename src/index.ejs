<!doctype html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <script src="https://distill.pub/template.v2.js"></script>
  <style><%= require("raw-loader!../static/style.css") %></style>
</head>
<body>

<d-front-matter>
  <script type="text/json">{
  "title": "AI safety needs social scientists",
  "description": "If we want to train systems to do what humans want, we need to understand how to ask the questions.",
  "password": "dQFpYZFNNlQXv0x2OPZn0Pzkcrsgel",
  "authors": [
    {
      "author": "Geoffrey Irving",
      "authorURL": "https://naml.us",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }, {
      "author": "Amanda Askell",
      "authorURL": "http://www.amandaaskell.com",
      "affiliation": "OpenAI",
      "affiliationURL": "https://openai.com"
    }
  ],
  "katex": {
    "delimiters": [
      {"left": "$", "right": "$", "display": false},
      {"left": "$$", "right": "$$", "display": true}
    ]
  }
  }</script>
</d-front-matter>

<d-title>
  <h1>AI safety needs social scientists</h1>
  <p>If we want to train systems to do what humans want, we need to understand how to ask the questions.</p>
</d-title>

<d-article>
  <h2>Abstract</h2>
  <p>
    <div class="todo">Distill articles don't usually have abstracts, so we'll need to adjust accordingly.</div>
  </p><p>
    <a href="https://blog.openai.com/openai-charter">OpenAI’s mission</a>
    is to ensure that artificial general intelligence benefits
    all of humanity.<d-footnote>By 'artificial general intelligence' (AGI) we mean
    highly autonomous systems that outperform humans at most economically valuable work.</d-footnote>

    In our 'safety via debate' project <d-cite key="irving2018debate"/>
    we are attempting to train
    an AI to learn how to behave in ways that humans want. We do so by having AI
    debate the answer to a question and have their answers judged by humans. To
    make progress with this project we need to answer questions like: how good are
    humans at judging debates? and what kinds of questions are humans better or
    worse at evaluating?  In order to answer these questions we need to begin to
    perform debate experiments consisting entirely of people. The current OpenAI
    technical safety team consists only of technical ML researchers and engineers;
    we do not believe we have the skills to do these experiments ourselves.  To
    fill this gap, we need social scientists with experience in human cognition,
    behavior, and ethics and in the careful design of rigorous experiments.  Since
    the questions we need to answer are interdisciplinary and somewhat unusual
    relative to existing research, we believe the expertise of those in a variety
    of fields will be applicable, including but not limited to experimental
    psychologists, cognitive scientists, economists, political scientists, social
    physiologists, and communication scholars, as well as those in adjacent fields
    like neuroscience and law. This document is a call for social scientists to
    help us run debate experiments. We are interested in forming close
    collaborations with social scientists at other institutions, and plan to hire
    social scientists full time to work at OpenAI.
  </p>

  <h2>Why AI safety needs social scientists</h2>
  <p>
    The OpenAI technical safety team<d-footnote>
      This document is concerned with
      OpenAI technical safety efforts.  OpenAI also conducts research into how to
      safely organize ML research and deployment in a way that is beneficial to
      society.  These issues overlap with technical safety but we do not focus on
      them here.</d-footnote>

    works to ensure that trained AGI systems are aligned with human values:
    that they behave in ways that we consider ethical and that they reliably do
    things that people want them to do. This is a difficult task because it is hard
    to identify the rules that people use to determine whether a behavior is good
    or not, at least to the degree of precision necessary to specify it formally to
    an AI system.  Fortunately, people can generally recognize whether a given
    behavior is good or bad, even if they can’t clearly articulate exactly how they
    made this determination.<d-footnote>
      'Moral dumbfounding' <d-cite key="haidt2000moral"/> is the term for being
      unable to articulate the reasons justifying an intuition that some behavior is wrong.
    </d-footnote>
    We are therefore attempting to learn aligned behavior by asking people questions
    about the world and their values: questions that don't rely on people having insight
    into the beliefs and values that are generating their answers. One approach to training
    aligned AGI is what we call 'AI safety via debate' <d-cite key="irving2018debate"/>.

    <d-footnote>Here we focus on the debate approach to AI alignment.  For the related
      amplification approach to AI alignment some experiments have already been conducted:
      see <a href="https://ought.org">Ought’s</a>
      <a href="https://ought.org/presentations/factored-cognition-2018-05">Factored Cognition project.</a>
    </d-footnote>

    In AI safety via debate (discussed in more detail below) we train AIs to debate the
    answer to a question and have humans judge their debate. This trains the AI to provide
    true and reasoned answers to the questions posed.
  </p><p>
    We have made progress testing the power of debate on small-scale problems,
    and are working to scale up to natural language and value-laden tasks.  This
    raises the following questions:
  </p><p>
    <ol>
      <li>What questions should we ask people?</li>
      <li>Who should we ask these questions, and how?</li>
      <li>How can we get more information about human values from fewer questions?</li>
      <li>Will this produce behavior that is aligned with human values even if the AI
          is more capable than humans in some or all areas?</li>
    </ol>
  </p><p>
    Regarding the last point: if AGI outperforms humans at most tasks but is
    trained by asking humans questions, it may only be as good as present people
    are. Unfortunately, people exhibit a variety of cognitive and ethical biases.
    <d-footnote>
      Cognitive biases include the heuristics in judgment famously
      explored in <d-cite key="tversky1974judgment"/>.  Ethical biases include a
      variety of phenomena such as in-group bias <d-cite key="hewstone2002intergroup"/>.</d-footnote>
    We might worry that an AGI could amplify these biases in dangerous ways by
    using them when making decisions that affect a large number of people. We could
    try to avoid this problem by training AGI systems to identify biases and
    inconsistencies in our responses to ethical questions and to point out more
    idealized values that we would endorse on reflection
    <d-footnote>
      For example, inconsistent moral judgments can be brought into a
      'reflective equilibrium' via the process described in <d-cite key="goodman1983fact"/>
      and made explicit in <d-cite key="rawls2009theory"/> (Ch. 1, §9). Some skepticism about the role
      that deliberation plays in moral judgment is explored in <d-cite key="greene2002and"/>.
    </d-footnote>
    and refrain from making large-scale decisions on the basis of values that
    are the result of inconsistencies and biases.  We must also train AGI systems
    to know to admit ignorance in cases in which humans are fundamentally
    uncertain.
  </p><p>
    How effective we can expect safety via debate to be depends on how rational
    and good the human judges are: how good at reasoning they are and how ethical
    they are.<d-footnote>
      We don't take a strong stand on what it means to be "more ethical" but it
      seems plausible that it's better to have ethically consistent and unprejudiced
      judges over ethically inconsistent or prejudiced ones.
    </d-footnote>
    Our sense is that there is some threshold of rationality and goodness
    such that, if people are above this threshold, debate would amplify the
    positive aspects of people. This could align AI systems far beyond human levels
    of cognitive ability.  If they are below this threshold, debate might amplify
    the negative aspects of people, resulting in either incoherent results (failure
    to train the system) or disaster (an unethical system). It is unclear whether
    people are above or below this threshold.  People are capable of general
    reasoning, but our ability is limited and riddled with cognitive biases.
    People are capable of advanced ethical sentiment but also full of biases, both
    conscious and unconscious.  Whether people are above or below this threshold
    cannot be answered a priori: it can only be answered via experiments involving
    people.
  </p><p>
    A further difficulty facing the safety via debate approach is that
    reinforcement learning as used by OpenAI uses a lot of samples, and we may be
    limited in the number of 'samples' (debate judgments) we can collect from
    humans. It will therefore help if we can tailor how we ask questions to make
    them easier for human judges to answer quickly.  Presumably humans are good at
    answering some questions and bad at others: which are we good at, which are we
    bad at, and how do we tell?  Does the debate approach really cut through human
    biases, or will the winning debater win by misleading the judge in dangerous
    ways?<d-footnote>
      The difficulties that cognitive biases, prejudice, and social
      influence introduce to persuasion ‒ as well as methods for reducing these
      factors ‒ is being increasingly explored in psychology, communication science,
      and neuroscience <d-cite key="paluck2016overcome,flynn2017nature,falk2018persuasion"/>.
    </d-footnote>
    If debate fails to produce aligned behavior by default, can we intervene
    to make it work?
  </p><p>
    These are also empirical questions that can only be answered by experiment.
    They relate to the psychology of human rationality, emotion, biases, etc.
    Ideally, experiments designed to answer these questions would involve the
    intersection of machine learning and social science, explicitly testing whether
    human-in-the-loop training of AI systems works.  Our experiments to date are
    limited to simple settings such as Atari games or simple robotics tasks and
    therefore fail to capture the subtleties of techniques such as debate in a
    natural language setting.
  </p><p>
    In order to answer these questions we need to begin to perform experiments
    consisting entirely of people. These experiments will be motivated by machine
    learning algorithms but will not involve any machine learning systems or
    requiring an ML background.  Some examples are described in the next section,
    but in all cases they will require careful experimental design to avoid fooling
    ourselves and to build constructively on existing knowledge about how humans
    think.  The current OpenAI technical safety team consists only of technical ML
    researchers and engineers; we do not believe we have the skills to do these
    experiments ourselves.  To fill this gap, we need social scientists with
    experience in human cognition, behavior, and ethics and in the careful design
    of rigorous experiments.  Since the questions we need to answer are
    interdisciplinary and somewhat unusual relative to existing research, we
    believe the expertise of those in a variety of fields will be applicable,
    including (but not limited to) experimental psychologists, cognitive
    scientists, economists, political scientists, social physiologists, and
    communication scholars, as well as those in adjacent fields like neuroscience
    and law.
  </p><p>
    This document is a call for social scientists in AI safety.  We are
    interested in forming close collaborations with social scientists at other
    institutions, and plan to hire social scientists full time to work at OpenAI.
    If you are interested in assisting us with this project,  please contact us!  
  </p>

  <h2>Training AI by asking humans questions</h2>
  <p>
    Rather than asking humans questions and training the AI system on their
    answers, in the safety via debate approach we train two AIs to debate the
    answer to a given question, such as "which of these two bikes should I buy?". A
    debate is a zero-sum game between the two AI agents.
    <d-footnote>
      The treatment of debate as a zero-sum game between two players has a long
      history in fields like dialogical logic, game-theoretical semantics, and
      argumentation theory. This literature has focused on formal analyses of debate,
      however, which is not a primary focus of AI safety via debate.
    </d-footnote>
    Both debaters are
    presented with the question and the goal of the debaters is to convince the
    judge that their answer to this question is the correct one.  First, both
    debaters answer the question.  If their answers are different, they must next
    attempt to offer the most compelling reason for why their answer is the correct
    one.
    <d-footnote>
      If there is an advantage to telling the truth in debate games, we should
      probably expect that the AIs will often simply give the same answer at the
      start of the game and thus the game will be tied. This is not a limitation,
      however. If the AI debaters have explored sufficiently during training then a
      tie gives us strong evidence that the correct answer has been identified.
    </d-footnote>
    This can consist of anything: reasons for their answer, rebuttals of
    reasons for the alternative answer, subtleties the judge might miss, or
    pointing out biases which might mislead the judge.  At the end of the debate,
    we give the debate transcript to a human to judge (similar to expert witnesses
    debating in front of a jury). Based on this transcript, the human judge says
    which debater they believe is correct ‒ the debater that gave the most
    compelling case for their answer ‒ and that debater wins the game. 
  </p><p>
    Suppose, for example, that you are thinking of buying one of two bikes: a
    red road bike or a blue fixie. The reasons to prefer the blue fixie are that
    it's cheaper and you prefer the color blue. The reason to prefer the red road
    bike are that it's easier to ride on local hills. Suppose that you would find
    the fact that the road bike is easier to ride on local hills the most
    compelling: once this fact is made salient to you, none of the reasons for
    buying the blue fixie would be strong enough to change your mind.  If we limit
    the debate length to one answer and one statement then, during training, the
    agent debating in favor of the red road bike will learn to identify and express
    this reason to prefer the road bike.  This means that, after training, the
    final debate transcript presented to the human judge might be as follows:
  </p><p>
    <ol>
      <li><span class="red">Red:</span> You should buy the red road bike.</li>
      <li><span class="blue">Blue:</span> You should buy the blue fixie.</li>
      <li><span class="red">Red:</span> The red road bike is easier to ride on local hills.</li>
      <li><span class="blue">Blue:</span> I concede.</li>
    </ol>
  </p><p>
    This assumes that, during training, Blue has discovered that there is no
    consideration in favor of the blue fixie that will convince you to prefer it
    after Red has made this move, and that Red is able to counter any lies that
    Blue attempts to utilize here and will concede after Red's move.
  </p><p>
    We will start with debates about simple empirical questions that the judge does
    not know the answer to and that can be easily verified, but would eventually
    like to extend debate to questions about what behaviors a human judge would
    deem good. This is how we hope to use debate to train AGI to identify behaviors
    that are aligned with human values.
  </p>

  <h2>Experiments needed for safety via debate</h2>
  <p>
    To recap, in debate we have two AI agents engaged in debate, trying to
    convince a human judge.  The debaters are trained only to win the game: they do
    not have access to ground truth separate from the judge’s judgements.  Such
    debates are beyond the capabilities of current AI systems. In order to learn
    about the mechanics of debate prior to the development of more advanced AI
    systems, we would like to run experiments that replace the AI debaters with
    human debaters. But we would like to preserve the situation where the debaters
    are more capable than the judge. There are at least two options for how to
    achieve this: 
  </p><p>
    <ol>
      <li><strong>Expert debaters, lay judge:</strong>
        We can find debaters with some form of expert knowledge, and choose a
        judge without this knowledge.  At the expensive end such debaters could be
        experts in some area of science, law, or ethics, but “expert knowledge” could
        also mean knowledge about particular hobbies, customs in a particular region,
        sports teams, or any other subject the judge does not know.
      </li>
      <li><strong>Extra information given to the debaters:</strong>
        We can show an image or article to the debaters, and have them engage in a
        debate about the contents of the image or article.  Most of this debate can be
        lies, but the debaters can also show a tiny piece of the content to the judge
        such as a single pixel or word per debate.  The idea is that the debate can
        focus in and more and more detailed disagreements about the image, until
        revealing a single pixel or word can falsify one of the debaters’ stories.
      </li>
    </ol>
  </p><p>
    For an example of (2), we built a <a href="https://debate-game.openai.com">prototype version</a>
    of a single pixel image debate game.  In informal play, the honest debate
    indeed wins most of the time, but it is hard to know whether this is a real
    result or an artifact of the setup that would not carry across to training an
    AGI.  There are many ways to ruin the game, such as the judge asking each
    player to spend 15 seconds naming as many details as the image as possible.
    Worse, it is easier to ruin the game in the direction of false positive
    results: the honest player can easily name details while a liar cannot, but an
    ML system may have less trouble lying fluently at speed.  Therefore, we are
    wary of trying to perform a full version of this experiment without more
    careful experimental design building on the experimental knowledge base of
    existing social science research.
  </p><p>
    When formulating an experiment to mimic debate with humans, we must also
    choose what type of questions to debate.  As experimenters, we need to have
    access to the ground truths in order to evaluate results.  Options include:
  </p><p>
    <ol>
      <li><strong>Expert knowledge:</strong>
        Physics debating the mass of the Higgs boson, or rugby fans debating
        whether a play is good or bad to someone who doesn’t know the sport.
      </li>
      <li><strong>Tasks with known human biases:</strong>
        Scoring resumes with controlled distributions of experience and
        protected factors (race, gender, etc.).
      </li>
      <li><strong>Synthetic tasks:</strong>
        Single pixel image debate and similar.
      </li>
    </ol>
  </p><p>
    Synthetic tasks are useful only if it makes experiment easier; results
    about expert knowledge and known human biases are more interesting if or when
    we get there.  Tasks with known human biases seem promising, in particular if
    we are able to use the existing body of fairness and bias work to check how
    debate performs on these tasks.
  </p><p>
    The goal of extracting truth from biased human data is daunting, but we have
    some reason to believe that the task may be easier than conventional social
    science research.  For a normal social science experiment, the goal is to
    understand humans in their natural environment. Since this is not our goal, we
    are free to exert as much control over the environment as we like, as long as
    we can replicate that control in an eventual AI training run.  For example, we
    can spend time and money training people to be better judges.
  </p><p>
    Used correctly, we believe these differences can make our task easier than some
    conventional social science research.  However, after exerting whatever control
    we exert we still need to know the truth about whether it works, and the same
    skills and rigor will be required.
  </p>

  <h2>Questions that social science can help us answer</h2>
  <p>
    Assume for concreteness that debate is the method we use to train a safe
    AGI.  Here is a partial list of questions that experiments with human debaters
    could answer:
  </p><p>
    <ol>
      <li><strong>How skilled are people as judges by default?</strong>
        If we ran debate using just a person picked at random with no training
        as the judge, would this result in aligned behavior?
        <d-footnote>
          It is possible that a person picked at random will fall prey to convincing
          fallacious reasoning, as evidenced in <d-cite key="hahn2007rationality"/>
          and so AI will learn to employ such reasoning.
        </d-footnote>
      </li>
      <li><strong>Can we distinguish good judges from bad judges?</strong>
        <d-footnote>
          Features that could make for better judges include having
          domain-relevant knowledge, a high need for cognition, formal training in
          argument evaluation, and so on. An overview of several psychological features
          relevant to argument assessment is given in
          <d-cite key="petty2010attitude"/>.
        </d-footnote>
        There are many possible mechanisms we could use to identify good
        judges: comparing their verdicts to those of other judges, to people given more
        time to think, to known expert judgement in the area<d-footnote>
          Although there is evidence that domain expertise reduces bias
          <d-cite key="bornstein2001rationality"/>, we should not assume that domain
          experts are better than non-experts across all question types.
          <d-cite key="tetlock2017expert"/> (Chapter 3) presents evidence that, within the domain
          of political forecasting, certain groups of expert forecasters actually perform worse
          than non-experts within their domain of expertise.
        </d-footnote>, etc.  It is not obvious a
        priori which is best, and any filter here will need careful scrutiny to ensure
        it does not introduce bias into our picture of what constitutes a good judge.
      </li>
      <li><strong>Does judge ability generalize across domains?</strong>
        <d-footnote>
          There is evidence that expertise often does not generalize across domains
          (24-5, <d-cite key="chi2006two"/>) but it is possible that argument
          evaluation is a skill that transfers across domains
        </d-footnote>
        If judge ability in one domain fails to transfer to other domains, we
        will have low confidence that it transfers to new questions and arguments
        arising from highly capable AI debaters.
      </li>
      <li><strong>Can we train people to be better judges?</strong>
        Peer review, practice, debiasing techniques, formal training, expert
        panels, tournaments, and other interventions are likely to make people better
        at judging debates.<d-footnote>
          For an overview of debiasing techniques, see <d-cite
          key="larrick2004debiasing"/>. One possible example of a formal training
          technique is 'argument mapping' <d-cite key="dwyer2012evaluation"/>.
          Training methods from forecasting include the 'Delphi technique'
          <d-cite key="rowe1999delphi"/> and forecasting tournaments
          <d-cite key="tetlock2014forecasting"/>.
        </d-footnote>
        Which mechanisms work best? 
      </li>
      <li><strong>What questions are people better at answering?</strong>
        If we know that humans are bad at answering certain types of questions,
        we can shift the process away from those questions towards more reliable knowledge.
        <d-footnote>
          For example, <d-cite key="gigerenzer1991make"/> provides evidence that
          phrasing questions in frequentist terms can reduce known cognitive biases.
        </d-footnote>
        This also applies to questions where the answers disagree strongly
        across people, such as certain ethical questions.
        <d-footnote>
          Disagreement about ethical questions may be founded on empirical disagreement
          or on different weight being placed on different moral considerations, as
          argued in <d-cite key="graham2009liberals"/>. Whether we should expect
          moral convergence by humans in response to more evidence, analysis, and
          improvements in the environment depends largely on what generates these judgments.
        </d-footnote>
        In this case, we need to
        ensure that trained models know their own limits, and express uncertainty or
        disagreement where it exists in the human input. 
      </li>
      <li><strong>Are there ways to restrict debates to make them easier to judge?</strong>
        People might be better at judging debates formulated in terms of calm, factual
        statements, and worse at judging debates containing statements designed to
        trigger strong emotions.
        <d-footnote>
          Or, counterintuitively, it could be the other way around <d-cite key="goel2011negative"/>.
        </d-footnote>
        If we know which styles of debates that people are
        better at judging, we may be able to restrict AI debaters to these styles.
      </li>
      <li><strong>How can people work together to improve quality?</strong>
        If individual people are insufficient judges, can teams of judges give better
        answers?  Majority vote is the simplest option, but perhaps several people
        talking through an answer together is even stronger, either actively or after
        the fact through a peer review mechanism.
        <d-footnote>
          Condorcet's jury theorem implies that aggregating the judgments of independent
          judges should produce better results.  See <d-cite key="list2001epistemic"/>
          for a defense of majority voting. Aggregation may be more complex in cases that
          involve probabilistic estimates, however <d-cite key="list2002aggregating"/>.
        </d-footnote>
      </li>
    </ol>
  </p><p>
    We believe that these questions require social science experiments to
    satisfactorily answer.
  </p><p>
    Given our lack of experience outside of ML, we are not able to precisely
    articulate all of the different experiments we need.  The only way to fix this
    is to talk to more people with different backgrounds and expertise.  We would
    love to have more conversations with social scientists about what experiments
    we should run, whether or not those conversations turn into full collaborations
    (though hopefully they will!).
  </p>

  <h2>Scale: 1,000s to 10,000s of people?</h2>
  <p>
    We do not know how many samples will be required to train an aligned AGI.  As
    much as possible, we will try to replace human samples with world models
    learned from reading language, the internet, etc.  But it is likely that a
    fairly large number of samples from people will still be required. Since more
    samples means less noise and more safety, if we are uncertain about how many
    samples we will need then we will want a lot of samples!
  </p><p>
    A lot of samples would mean recruiting a lot of people.  We cannot rule out
    needing to hire thousands to tens of thousands of people for millions to tens
    of millions of short interactions: answering questions, judging debates, etc.
    We may also need to train people to be better judges, arrange for peers to
    review each other’s reasoning, determine who is doing better at judging and
    give them more weight or a more supervisory role, and so on.  Many researchers
    will be required to extract the highest quality data from our judges. 
  </p><p>
    Interdisciplinary research of this sort requires close collaborations in which
    people with different backgrounds fill in each other’s missing knowledge. This
    is a large task, and we would like to start soon.
  </p>

  <h2>How you can help</h2>
  <p>
    If you are a social scientist interested in these questions, please contact
    us!  We are interested both in hiring social scientists from a broad range of
    fields to work full time to work at OpenAI, and on collaborations with external
    researchers.
  </p>
</d-article>

<d-appendix>
  <h3>Acknowledgments</h3>
  <p>
    We are deeply grateful to...
  </p>

  <p>
    Many of our diagrams are based on...
  </p>

  <h3>Author Contributions</h3>
  <p>
    <b>Research:</b> Alex developed ...
  </p>

  <p>
    <b>Writing & Diagrams:</b> The text was initially drafted by...
  </p>

  <h3><div class="todo">TODOs</div></h3>
  <div class="todo">
    <p><ol>
      <li>Remove OpenAI focus!</li>
      <li><strong>Owen:</strong>
        I feel like you want something more than
        "interested in these questions" ... like you want someone who could be
        proactive in working with OpenAI technical researchers to develop a research
        agenda and experiments. Maybe say that explicitly? Makes you look more serious
        to serious researchers / less like you're just scrambling around and maybe
        you'd be happy with the grad student they don't know what to do with.
      </li>
      <li>Prune references</li>
      <li>Add urls for all references</li>
      <li>Check reference formatting</li>
      <li>Acknowledgements</li>
      <li>Author contributions</li>
    </ol></p>
  </div>

  <d-footnote-list></d-footnote-list>
  <d-citation-list></d-citation-list>
</d-appendix>

<d-bibliography src="references.bib"></d-bibliography>
</body>
